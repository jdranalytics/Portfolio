{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa62fd15",
   "metadata": {},
   "source": [
    "## APLICACIÓN DE MODELOS DE REGLOG Y KMEANS PARA PREDICCIÓN DE RIESGO Y CLUSTERING\n",
    "\n",
    "#### NOTEBOOK: Aplicar_Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f161e371",
   "metadata": {},
   "source": [
    "![Secuencia de Modelos ML](../Imagenes/Secuencia%20de%20Modelos%20ML.png)\n",
    "\n",
    "Flujo de ejecución automatizada de modelos de machine learning en MS Fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e7f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.clustering import KMeansModel\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.sql.functions import col, when, count, isnan, isnull\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, StringType, DoubleType\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Verificar y obtener ruta del lakehouse\n",
    "lakehouse_paths = [\n",
    "    \"Tables\",  # Ruta relativa\n",
    "    \"/lakehouse/default/Tables\",  # Ruta absoluta\n",
    "    \"abfss://lakehouse@onelake.dfs.fabric.microsoft.com/Tables\"  # URL ABFS\n",
    "]\n",
    "\n",
    "def test_lakehouse_access(path):\n",
    "    try:\n",
    "        test_path = f\"{path}/test_aplicacion\"\n",
    "        test_df = spark.createDataFrame([(1,)], [\"test\"])\n",
    "        test_df.write.mode(\"overwrite\").format(\"delta\").save(test_path)\n",
    "        print(f\"Escritura exitosa en {test_path}\")\n",
    "        return True, test_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error al escribir en {test_path}: {str(e)}\")\n",
    "        return False, None\n",
    "\n",
    "lakehouse_path = None\n",
    "for path in lakehouse_paths:\n",
    "    success, test_path = test_lakehouse_access(path)\n",
    "    if success:\n",
    "        lakehouse_path = path\n",
    "        break\n",
    "\n",
    "if not lakehouse_path:\n",
    "    raise Exception(\"No se pudo acceder a ninguna ruta del lakehouse. Verifica los permisos y la configuración.\")\n",
    "\n",
    "print(f\"Usando ruta del lakehouse: {lakehouse_path}\")\n",
    "\n",
    "# Función para registrar logs\n",
    "def log_reentrenamiento(step_name, status, metrics=None, error_message=None):\n",
    "    log_schema = StructType([\n",
    "        StructField(\"fecha_ejecucion\", TimestampType(), False),\n",
    "        StructField(\"paso\", StringType(), False),\n",
    "        StructField(\"estado\", StringType(), False),\n",
    "        StructField(\"error_mensaje\", StringType(), True),\n",
    "        StructField(\"metricas\", StringType(), True)\n",
    "    ])\n",
    "    log_df = spark.createDataFrame([(\n",
    "        datetime.now(),\n",
    "        step_name,\n",
    "        status,\n",
    "        error_message,\n",
    "        str(metrics) if metrics else None\n",
    "    )], schema=log_schema)\n",
    "    try:\n",
    "        log_df.write.mode(\"append\").format(\"delta\").save(f\"{lakehouse_path}/reentrenamiento_log\")\n",
    "        print(f\"Log registrado para {step_name}: {status}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al registrar log: {e}\")\n",
    "\n",
    "# Función para notificaciones Discord\n",
    "discord_webhook_url = os.getenv('DISCORD_WEBHOOK_URL', \"https://discord.com/api/webhooks/1362094514688360518/NZQcmWfVB3Isx1hbSC1_g2R-gDWmhUwaszrWZA21zHCpKcco2BHFHhOSf_tOzvzi11_p\")\n",
    "\n",
    "def crear_mensaje(step_name, status, metrics=None, error_message=None):\n",
    "    color = 3066993 if status == \"Éxito\" else 15158332\n",
    "    description = f\"Aplicación de {step_name} completada con estado: {status}\"\n",
    "    if error_message:\n",
    "        description += f\"\\nError: {error_message}\"\n",
    "    \n",
    "    embed = {\n",
    "        \"title\": f\"{'✅' if status == 'Éxito' else '❌'} Aplicación de Modelos - {step_name}\",\n",
    "        \"description\": description,\n",
    "        \"color\": color,\n",
    "        \"footer\": {\"text\": \"RiskApp - Sistema de Monitoreo\"},\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    if metrics:\n",
    "        embed[\"fields\"] = [\n",
    "            {\"name\": key, \"value\": f\"{value:.4f}\", \"inline\": True}\n",
    "            for key, value in metrics.items()\n",
    "        ]\n",
    "    \n",
    "    return {\"embeds\": [embed]}\n",
    "\n",
    "def enviar_notificacion(mensaje):\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                discord_webhook_url, \n",
    "                json=mensaje,\n",
    "                timeout=10  # Agregar timeout\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            print(f\"Notificación enviada para {mensaje['embeds'][0]['title']}\")\n",
    "            return\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt == max_retries - 1:  # Si es el último intento\n",
    "                print(f\"Error al enviar notificación después de {max_retries} intentos: {str(e)}\")\n",
    "            else:\n",
    "                print(f\"Intento {attempt + 1} fallido, reintentando...\")\n",
    "                time.sleep(1)  # Esperar 1 segundo antes de reintentar\n",
    "\n",
    "# Cargar y preparar datos\n",
    "try:\n",
    "    df = spark.read.format(\"delta\").load(f\"{lakehouse_path}/solicitudes_processed\")\n",
    "    total_records = df.count()\n",
    "    \n",
    "    # Verificar valores nulos por tipo de dato\n",
    "    null_counts = {}\n",
    "    for col_name, dtype in df.dtypes:\n",
    "        if dtype in (\"double\", \"float\"):\n",
    "            null_count = df.filter(col(col_name).isNull() | isnan(col(col_name))).count()\n",
    "        else:\n",
    "            null_count = df.filter(col(col_name).isNull()).count()\n",
    "        null_counts[col_name] = null_count\n",
    "    \n",
    "    print(f\"Datos cargados. Total de registros: {total_records}\")\n",
    "    print(\"\\nAnálisis de valores nulos:\")\n",
    "    for col_name, null_count in null_counts.items():\n",
    "        null_percentage = (null_count / total_records) * 100\n",
    "        print(f\"{col_name}: {null_count} nulos ({null_percentage:.2f}%)\")\n",
    "    \n",
    "    # Imputar valores nulos con la mediana\n",
    "    numeric_cols = [\"edad\", \"ingresos_anuales\", \"puntaje_crediticio\", \"deuda_actual\", \"antiguedad_laboral\"]\n",
    "    for col_name in numeric_cols:\n",
    "        median_value = df.approxQuantile(col_name, [0.5], 0.01)[0]\n",
    "        df = df.fillna(median_value, subset=[col_name])\n",
    "    \n",
    "    # Rellenar valores nulos en columnas codificadas\n",
    "    encoded_cols = [\"historial_pagos_encoded\", \"estado_civil_encoded\", \"tipo_empleo_encoded\"]\n",
    "    df = df.fillna(0, subset=encoded_cols)\n",
    "    \n",
    "    log_reentrenamiento(\"Carga_datos\", \"Éxito\", metrics={\"total_records\": total_records})\n",
    "    mensaje = crear_mensaje(\"Carga_datos\", \"Éxito\", metrics={\"total_records\": float(total_records)})\n",
    "    enviar_notificacion(mensaje)\n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    log_reentrenamiento(\"Carga_datos\", \"Error\", error_message=error_msg)\n",
    "    mensaje = crear_mensaje(\"Carga_datos\", \"Error\", error_message=error_msg)\n",
    "    enviar_notificacion(mensaje)\n",
    "    raise Exception(f\"Error al cargar datos: {error_msg}\")\n",
    "\n",
    "# Aplicar clustering\n",
    "try:\n",
    "    print(\"Cargando modelo K-means...\")\n",
    "    kmeans_model = KMeansModel.load(f\"{lakehouse_path}/Models/KMeansClustering\")\n",
    "    \n",
    "    # Preparar características para clustering\n",
    "    feature_cols = [\"edad\", \"ingresos_anuales\", \"puntaje_crediticio\", \"deuda_actual\",\n",
    "                   \"antiguedad_laboral\", \"historial_pagos_encoded\", \"estado_civil_encoded\",\n",
    "                   \"tipo_empleo_encoded\", \"numero_dependientes\"]\n",
    "    \n",
    "    # Configurar preprocesamiento\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=feature_cols,\n",
    "        outputCol=\"features\",\n",
    "        handleInvalid=\"skip\"\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler(\n",
    "        inputCol=\"features\",\n",
    "        outputCol=\"scaled_features\",\n",
    "        withStd=True,\n",
    "        withMean=True\n",
    "    )\n",
    "    \n",
    "    print(\"Aplicando transformación de clustering...\")\n",
    "    # Aplicar preprocesamiento y clustering\n",
    "    assembled_df = assembler.transform(df)\n",
    "    scaled_df = scaler.fit(assembled_df).transform(assembled_df)\n",
    "    clustered_df = kmeans_model.transform(scaled_df)\n",
    "    \n",
    "    # Analizar resultados\n",
    "    cluster_distribution = clustered_df.groupBy(\"prediction\").count().toPandas()\n",
    "    print(\"\\nDistribución de clusters:\")\n",
    "    print(cluster_distribution)\n",
    "    \n",
    "    # Guardar resultados - Corregido para usar id_cliente\n",
    "    result_df = clustered_df.select(\n",
    "        \"id_cliente\",  # Cambiado de id_solicitud a id_cliente\n",
    "        \"estado_solicitud\",\n",
    "        col(\"prediction\").alias(\"cluster\")\n",
    "    )\n",
    "    \n",
    "    result_df.write.mode(\"overwrite\").format(\"delta\").save(f\"{lakehouse_path}/solicitudes_clustered\")\n",
    "    \n",
    "    metrics = {\n",
    "        \"total_clustered\": float(result_df.count()),\n",
    "        \"num_clusters\": float(cluster_distribution.shape[0])\n",
    "    }\n",
    "    \n",
    "    # Mejorar manejo de notificaciones\n",
    "    try:\n",
    "        log_reentrenamiento(\"Clustering\", \"Éxito\", metrics=metrics)\n",
    "        mensaje = crear_mensaje(\"Clustering\", \"Éxito\", metrics=metrics)\n",
    "        enviar_notificacion(mensaje)\n",
    "    except Exception as notif_error:\n",
    "        print(f\"Error al enviar notificación: {str(notif_error)}\")\n",
    "        # Continuar con el proceso aunque falle la notificación\n",
    "        \n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    try:\n",
    "        log_reentrenamiento(\"Clustering\", \"Error\", error_message=error_msg)\n",
    "        mensaje = crear_mensaje(\"Clustering\", \"Error\", error_message=error_msg)\n",
    "        enviar_notificacion(mensaje)\n",
    "    except Exception as notif_error:\n",
    "        print(f\"Error al enviar notificación de error: {str(notif_error)}\")\n",
    "    raise Exception(f\"Error al aplicar clustering: {error_msg}\")\n",
    "\n",
    "# Aplicar clasificación\n",
    "try:\n",
    "    print(\"Cargando modelo de clasificación...\")\n",
    "    clf_model = PipelineModel.load(f\"{lakehouse_path}/Models/ClasificadorCredito\")\n",
    "    print(\"Filtrando solicitudes pendientes...\")\n",
    "    pending_df = df.filter(col(\"solicitud_credito\").isNull())\n",
    "    pending_count = pending_df.count()\n",
    "    \n",
    "    if pending_count > 0:\n",
    "        print(f\"Procesando {pending_count} solicitudes pendientes...\")\n",
    "        predictions = clf_model.transform(pending_df)\n",
    "        \n",
    "        # Analizar predicciones\n",
    "        pred_distribution = predictions.groupBy(\"prediction\").count().toPandas()\n",
    "        print(\"\\nDistribución de predicciones:\")\n",
    "        print(pred_distribution)\n",
    "        \n",
    "        # Preparar resultados finales\n",
    "        from pyspark.sql.functions import udf, avg\n",
    "        from pyspark.sql.types import DoubleType\n",
    "        \n",
    "        # UDF para extraer el score de aprobación\n",
    "        def get_probability_class_1(prob_vector):\n",
    "            return float(prob_vector.values[1]) if len(prob_vector.values) > 1 else 0.0\n",
    "        \n",
    "        get_prob_udf = udf(get_probability_class_1, DoubleType())\n",
    "        \n",
    "        final_predictions = predictions.select(\n",
    "            \"id_cliente\",\n",
    "            \"estado_solicitud\",\n",
    "            col(\"prediction\").cast(\"double\"),\n",
    "            \"probability\",\n",
    "            when(col(\"prediction\") == 1.0, \"Aprobado\")\n",
    "            .otherwise(\"Rechazado\").alias(\"prediccion_aprobado\"),\n",
    "            get_prob_udf(\"probability\").alias(\"score_aprobacion\")\n",
    "        )\n",
    "        \n",
    "        final_predictions.write.mode(\"overwrite\").format(\"delta\").save(f\"{lakehouse_path}/predicciones_pendientes\")\n",
    "        \n",
    "        # Calcular métricas usando funciones de agregación de PySpark\n",
    "        avg_score = final_predictions.agg(avg(\"score_aprobacion\").alias(\"avg_score\")).collect()[0][\"avg_score\"]\n",
    "        \n",
    "        metrics = {\n",
    "            \"total_pendientes\": float(pending_count),\n",
    "            \"aprobados_predichos\": float(pred_distribution[pred_distribution[\"prediction\"] == 1.0][\"count\"].iloc[0]),\n",
    "            \"score_promedio\": float(avg_score)\n",
    "        }\n",
    "        \n",
    "        log_reentrenamiento(\"Clasificación\", \"Éxito\", metrics=metrics)\n",
    "        mensaje = crear_mensaje(\"Clasificación\", \"Éxito\", metrics=metrics)\n",
    "    else:\n",
    "        mensaje = crear_mensaje(\"Clasificación\", \"Éxito\", error_message=\"No hay solicitudes pendientes\")\n",
    "        log_reentrenamiento(\"Clasificación\", \"Éxito\", error_message=\"No hay solicitudes pendientes\")\n",
    "    \n",
    "    enviar_notificacion(mensaje)\n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    log_reentrenamiento(\"Clasificación\", \"Error\", error_message=error_msg)\n",
    "    mensaje = crear_mensaje(\"Clasificación\", \"Error\", error_message=error_msg)\n",
    "    enviar_notificacion(mensaje)\n",
    "    raise Exception(f\"Error al aplicar clasificación: {error_msg}\")\n",
    "\n",
    "# Notificación final\n",
    "try:\n",
    "    mensaje = crear_mensaje(\n",
    "        \"Proceso Completo\",\n",
    "        \"Éxito\",\n",
    "        metrics={\n",
    "            \"total_registros\": float(total_records),\n",
    "            \"solicitudes_pendientes\": float(pending_count if 'pending_count' in locals() else 0)\n",
    "        }\n",
    "    )\n",
    "    enviar_notificacion(mensaje)\n",
    "except Exception as e:\n",
    "    print(f\"Error al enviar notificación final: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86d0de7",
   "metadata": {},
   "source": [
    "## Respuesta de aplicación automatizada de modelos en MS Fabric\n",
    "\n",
    "![Respuesta de Aplicacion de Modelos](../Imagenes/Respuesta%20de%20Aplicacion%20de%20Modelos.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
