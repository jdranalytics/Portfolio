{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa62fd15",
   "metadata": {},
   "source": [
    "## APLICACIÓN DE MODELOS DE REGLOG Y KMEANS PARA PREDICCIÓN DE RIESGO Y CLUSTERING\n",
    "\n",
    "#### NOTEBOOK: Aplicar_Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f161e371",
   "metadata": {},
   "source": [
    "![Secuencia de Modelos ML](../Imagenes/01_Secuencia%20de%20Modelos%20ML.png)\n",
    "\n",
    "Flujo de ejecución automatizada de modelos de machine learning en MS Fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e7f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.clustering import KMeansModel\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.sql.functions import col, when, count, isnan, isnull, avg, udf\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, StringType, DoubleType\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Definir UDF para extraer probabilidad\n",
    "@udf(returnType=DoubleType())\n",
    "def get_prob_udf(probability_vector):\n",
    "    \"\"\"\n",
    "    Extrae la probabilidad de aprobación (clase 1.0) del vector de probabilidades\n",
    "    Args:\n",
    "        probability_vector: Vector de probabilidades [p(0), p(1)]\n",
    "    Returns:\n",
    "        float: Probabilidad de aprobación (clase 1.0)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # El vector de probabilidad tiene el formato [p(0), p(1)]\n",
    "        # Retornamos p(1) que es la probabilidad de aprobación\n",
    "        return float(probability_vector[1])\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Verificar y obtener ruta del lakehouse\n",
    "lakehouse_paths = [\n",
    "    \"Tables\",  # Ruta relativa\n",
    "    \"/lakehouse/default/Tables\",  # Ruta absoluta\n",
    "    \"abfss://lakehouse@onelake.dfs.fabric.microsoft.com/Tables\"  # URL ABFS\n",
    "]\n",
    "\n",
    "def test_lakehouse_access(path):\n",
    "    try:\n",
    "        test_path = f\"{path}/test_aplicacion\"\n",
    "        test_df = spark.createDataFrame([(1,)], [\"test\"])\n",
    "        test_df.write.mode(\"overwrite\").format(\"delta\").save(test_path)\n",
    "        print(f\"Escritura exitosa en {test_path}\")\n",
    "        return True, test_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error al escribir en {test_path}: {str(e)}\")\n",
    "        return False, None\n",
    "\n",
    "lakehouse_path = None\n",
    "for path in lakehouse_paths:\n",
    "    success, test_path = test_lakehouse_access(path)\n",
    "    if success:\n",
    "        lakehouse_path = path\n",
    "        break\n",
    "\n",
    "if not lakehouse_path:\n",
    "    raise Exception(\"No se pudo acceder a ninguna ruta del lakehouse. Verifica los permisos y la configuración.\")\n",
    "\n",
    "print(f\"Usando ruta del lakehouse: {lakehouse_path}\")\n",
    "\n",
    "# Función para registrar logs\n",
    "def log_reentrenamiento(step_name, status, metrics=None, error_message=None):\n",
    "    log_schema = StructType([\n",
    "        StructField(\"fecha_ejecucion\", TimestampType(), False),\n",
    "        StructField(\"paso\", StringType(), False),\n",
    "        StructField(\"estado\", StringType(), False),\n",
    "        StructField(\"error_mensaje\", StringType(), True),\n",
    "        StructField(\"metricas\", StringType(), True)\n",
    "    ])\n",
    "    log_df = spark.createDataFrame([(\n",
    "        datetime.now(),\n",
    "        step_name,\n",
    "        status,\n",
    "        error_message,\n",
    "        str(metrics) if metrics else None\n",
    "    )], schema=log_schema)\n",
    "    try:\n",
    "        log_df.write.mode(\"append\").format(\"delta\").save(f\"{lakehouse_path}/reentrenamiento_log\")\n",
    "        print(f\"Log registrado para {step_name}: {status}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al registrar log: {e}\")\n",
    "\n",
    "# Función para notificaciones Discord\n",
    "discord_webhook_url = os.getenv('DISCORD_WEBHOOK_URL', \"XXXXXXXXXXXXXXXXXXXX\")\n",
    "\n",
    "def crear_mensaje(step_name, status, metrics=None, error_message=None):\n",
    "    color = 3066993 if status == \"Éxito\" else 15158332\n",
    "    description = f\"Aplicación de {step_name} completada con estado: {status}\"\n",
    "    if error_message:\n",
    "        description += f\"\\nError: {error_message}\"\n",
    "    \n",
    "    embed = {\n",
    "        \"title\": f\"{'✅' if status == 'Éxito' else '❌'} Aplicación de Modelos - {step_name}\",\n",
    "        \"description\": description,\n",
    "        \"color\": color,\n",
    "        \"footer\": {\"text\": \"RiskApp - Sistema de Monitoreo\"},\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    if metrics:\n",
    "        embed[\"fields\"] = [\n",
    "            {\"name\": key, \"value\": f\"{value:.4f}\", \"inline\": True}\n",
    "            for key, value in metrics.items()\n",
    "        ]\n",
    "    \n",
    "    return {\"embeds\": [embed]}\n",
    "\n",
    "def enviar_notificacion(mensaje):\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                discord_webhook_url, \n",
    "                json=mensaje,\n",
    "                timeout=10  # Agregar timeout\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            print(f\"Notificación enviada para {mensaje['embeds'][0]['title']}\")\n",
    "            return\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt == max_retries - 1:  # Si es el último intento\n",
    "                print(f\"Error al enviar notificación después de {max_retries} intentos: {str(e)}\")\n",
    "            else:\n",
    "                print(f\"Intento {attempt + 1} fallido, reintentando...\")\n",
    "                time.sleep(1)  # Esperar 1 segundo antes de reintentar\n",
    "\n",
    "# Función para verificar existencia y permisos de tabla Delta\n",
    "def check_delta_table(path, table_name):\n",
    "    try:\n",
    "        # Intentar leer la tabla\n",
    "        spark.read.format(\"delta\").load(f\"{path}/{table_name}\")\n",
    "        return True, None\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error accediendo a la tabla {table_name}: {str(e)}\"\n",
    "        return False, error_msg\n",
    "\n",
    "# Función para validar el esquema y tipos de datos\n",
    "def validate_table_schema(df, table_name):\n",
    "    try:\n",
    "        # Validar tipos de datos para compatibilidad SQL\n",
    "        for column, dtype in df.dtypes:\n",
    "            if 'vector' in str(dtype).lower() or 'array' in str(dtype).lower():\n",
    "                raise Exception(f\"Columna {column} tiene tipo {dtype} no compatible con SQL\")\n",
    "        return True, None\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error de validación de esquema para {table_name}: {str(e)}\"\n",
    "        return False, error_msg\n",
    "\n",
    "# Función mejorada para escribir en tabla Delta\n",
    "def write_delta_table_with_validation(df, path, table_name, mode=\"overwrite\", max_retries=3):\n",
    "    # Validar esquema primero\n",
    "    schema_valid, schema_error = validate_table_schema(df, table_name)\n",
    "    if not schema_valid:\n",
    "        return False, schema_error\n",
    "    \n",
    "    # Asegurar que los tipos de datos sean compatibles con SQL\n",
    "    try:\n",
    "        # Convertir columnas específicas a tipos compatibles\n",
    "        if table_name == \"predicciones_pendientes\":\n",
    "            df = df.select(\n",
    "                \"id_cliente\",\n",
    "                \"estado_solicitud\",\n",
    "                col(\"prediction\").cast(\"double\"),\n",
    "                col(\"prediccion_aprobado\").cast(\"string\"),\n",
    "                col(\"score_aprobacion\").cast(\"double\")\n",
    "            )\n",
    "    except Exception as e:\n",
    "        return False, f\"Error al convertir tipos de datos: {str(e)}\"\n",
    "    \n",
    "    # Intentar escribir con reintentos\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            df.write.mode(mode) \\\n",
    "                .format(\"delta\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .option(\"mergeSchema\", \"true\") \\\n",
    "                .save(f\"{path}/{table_name}\")\n",
    "            return True, None\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                error_msg = f\"Error escribiendo en tabla {table_name} después de {max_retries} intentos: {str(e)}\"\n",
    "                return False, error_msg\n",
    "            time.sleep(2 ** attempt)\n",
    "\n",
    "# Función mejorada para notificaciones Discord\n",
    "def enviar_notificacion_tabla(table_name, status, error_message=None):\n",
    "    color = 3066993 if status == \"Éxito\" else 15158332\n",
    "    description = f\"Operación en tabla {table_name}: {status}\"\n",
    "    if error_message:\n",
    "        description += f\"\\nError: {error_message}\"\n",
    "    \n",
    "    embed = {\n",
    "        \"title\": f\"{'✅' if status == 'Éxito' else '❌'} Estado Tabla - {table_name}\",\n",
    "        \"description\": description,\n",
    "        \"color\": color,\n",
    "        \"footer\": {\"text\": \"RiskApp - Sistema de Monitoreo de Tablas\"},\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            discord_webhook_url,\n",
    "            json={\"embeds\": [embed]},\n",
    "            timeout=10\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        print(f\"Notificación enviada para tabla {table_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al enviar notificación para tabla {table_name}: {str(e)}\")\n",
    "\n",
    "# Verificar todas las tablas necesarias al inicio\n",
    "required_tables = [\"solicitudes_processed\", \"predicciones_pendientes\", \"solicitudes_clustered\"]\n",
    "table_status = {}\n",
    "\n",
    "for table in required_tables:\n",
    "    success, error = check_delta_table(lakehouse_path, table)\n",
    "    table_status[table] = {\"exists\": success, \"error\": error}\n",
    "    if not success:\n",
    "        enviar_notificacion_tabla(table, \"Error\", error)\n",
    "\n",
    "# Cargar y preparar datos\n",
    "try:\n",
    "    df = spark.read.format(\"delta\").load(f\"{lakehouse_path}/solicitudes_processed\")\n",
    "    total_records = df.count()\n",
    "    \n",
    "    # Verificar valores nulos por tipo de dato\n",
    "    null_counts = {}\n",
    "    for col_name, dtype in df.dtypes:\n",
    "        if dtype in (\"double\", \"float\"):\n",
    "            null_count = df.filter(col(col_name).isNull() | isnan(col(col_name))).count()\n",
    "        else:\n",
    "            null_count = df.filter(col(col_name).isNull()).count()\n",
    "        null_counts[col_name] = null_count\n",
    "    \n",
    "    print(f\"Datos cargados. Total de registros: {total_records}\")\n",
    "    print(\"\\nAnálisis de valores nulos:\")\n",
    "    for col_name, null_count in null_counts.items():\n",
    "        null_percentage = (null_count / total_records) * 100\n",
    "        print(f\"{col_name}: {null_count} nulos ({null_percentage:.2f}%)\")\n",
    "    \n",
    "    # Imputar valores nulos con la mediana\n",
    "    numeric_cols = [\"edad\", \"ingresos_anuales\", \"puntaje_crediticio\", \"deuda_actual\", \"antiguedad_laboral\"]\n",
    "    for col_name in numeric_cols:\n",
    "        median_value = df.approxQuantile(col_name, [0.5], 0.01)[0]\n",
    "        df = df.fillna(median_value, subset=[col_name])\n",
    "    \n",
    "    # Rellenar valores nulos en columnas codificadas\n",
    "    encoded_cols = [\"historial_pagos_encoded\", \"estado_civil_encoded\", \"tipo_empleo_encoded\"]\n",
    "    df = df.fillna(0, subset=encoded_cols)\n",
    "    \n",
    "    log_reentrenamiento(\"Carga_datos\", \"Éxito\", metrics={\"total_records\": total_records})\n",
    "    mensaje = crear_mensaje(\"Carga_datos\", \"Éxito\", metrics={\"total_records\": float(total_records)})\n",
    "    enviar_notificacion(mensaje)\n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    log_reentrenamiento(\"Carga_datos\", \"Error\", error_message=error_msg)\n",
    "    mensaje = crear_mensaje(\"Carga_datos\", \"Error\", error_message=error_msg)\n",
    "    enviar_notificacion(mensaje)\n",
    "    raise Exception(f\"Error al cargar datos: {error_msg}\")\n",
    "\n",
    "# Aplicar clustering\n",
    "try:\n",
    "    print(\"Cargando modelo K-means...\")\n",
    "    kmeans_model = KMeansModel.load(f\"{lakehouse_path}/Models/KMeansClustering\")\n",
    "    \n",
    "    # Preparar características para clustering\n",
    "    feature_cols = [\"edad\", \"ingresos_anuales\", \"puntaje_crediticio\", \"deuda_actual\",\n",
    "                   \"antiguedad_laboral\", \"historial_pagos_encoded\", \"estado_civil_encoded\",\n",
    "                   \"tipo_empleo_encoded\", \"numero_dependientes\"]\n",
    "    \n",
    "    # Configurar preprocesamiento\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=feature_cols,\n",
    "        outputCol=\"features\",\n",
    "        handleInvalid=\"skip\"\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler(\n",
    "        inputCol=\"features\",\n",
    "        outputCol=\"scaled_features\",\n",
    "        withStd=True,\n",
    "        withMean=True\n",
    "    )\n",
    "    \n",
    "    print(\"Aplicando transformación de clustering...\")\n",
    "    # Aplicar preprocesamiento y clustering\n",
    "    assembled_df = assembler.transform(df)\n",
    "    scaled_df = scaler.fit(assembled_df).transform(assembled_df)\n",
    "    clustered_df = kmeans_model.transform(scaled_df)\n",
    "    \n",
    "    # Analizar resultados\n",
    "    cluster_distribution = clustered_df.groupBy(\"prediction\").count().toPandas()\n",
    "    print(\"\\nDistribución de clusters:\")\n",
    "    print(cluster_distribution)\n",
    "    \n",
    "    # Guardar resultados - Corregido para usar id_cliente\n",
    "    result_df = clustered_df.select(\n",
    "        \"id_cliente\",  # Cambiado de id_solicitud a id_cliente\n",
    "        \"estado_solicitud\",\n",
    "        col(\"prediction\").alias(\"cluster\")\n",
    "    )\n",
    "    \n",
    "    success, error = write_delta_table_with_validation(result_df, lakehouse_path, \"solicitudes_clustered\")\n",
    "    if not success:\n",
    "        raise Exception(error)\n",
    "    \n",
    "    verify_success, verify_error = check_delta_table(lakehouse_path, \"solicitudes_clustered\")\n",
    "    if not verify_success:\n",
    "        raise Exception(f\"Error verificando tabla después de escritura: {verify_error}\")\n",
    "    \n",
    "    metrics = {\n",
    "        \"total_clustered\": float(result_df.count()),\n",
    "        \"num_clusters\": float(cluster_distribution.shape[0])\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        log_reentrenamiento(\"Clustering\", \"Éxito\", metrics=metrics)\n",
    "        mensaje = crear_mensaje(\"Clustering\", \"Éxito\", metrics=metrics)\n",
    "        enviar_notificacion(mensaje)\n",
    "        enviar_notificacion_tabla(\"solicitudes_clustered\", \"Éxito\")\n",
    "    except Exception as notif_error:\n",
    "        print(f\"Error al enviar notificación: {str(notif_error)}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    try:\n",
    "        log_reentrenamiento(\"Clustering\", \"Error\", error_message=error_msg)\n",
    "        mensaje = crear_mensaje(\"Clustering\", \"Error\", error_message=error_msg)\n",
    "        enviar_notificacion(mensaje)\n",
    "        enviar_notificacion_tabla(\"solicitudes_clustered\", \"Error\", error_msg)\n",
    "    except Exception as notif_error:\n",
    "        print(f\"Error al enviar notificación de error: {str(notif_error)}\")\n",
    "    raise Exception(f\"Error al aplicar clustering: {error_msg}\")\n",
    "\n",
    "# Antes de la sección de clasificación, agregar función de validación SQL\n",
    "def ensure_sql_compatibility(df, table_name):\n",
    "    \"\"\"\n",
    "    Asegura que los tipos de datos sean compatibles con SQL Server\n",
    "    \"\"\"\n",
    "    sql_compatible_schema = {\n",
    "        \"predicciones_pendientes\": {\n",
    "            \"id_cliente\": \"string\",\n",
    "            \"estado_solicitud\": \"string\",\n",
    "            \"prediction\": \"double\",\n",
    "            \"prediccion_aprobado\": \"string\",\n",
    "            \"score_aprobacion\": \"double\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if table_name in sql_compatible_schema:\n",
    "        required_schema = sql_compatible_schema[table_name]\n",
    "        for col_name, required_type in required_schema.items():\n",
    "            if col_name not in df.columns:\n",
    "                raise ValueError(f\"Columna requerida {col_name} no encontrada en el DataFrame\")\n",
    "            df = df.withColumn(col_name, col(col_name).cast(required_type))\n",
    "    return df\n",
    "\n",
    "# Aplicar clasificación\n",
    "try:\n",
    "    print(\"Cargando modelo de clasificación...\")\n",
    "    clf_model = PipelineModel.load(f\"{lakehouse_path}/Models/ClasificadorCredito\")\n",
    "    print(\"Filtrando solicitudes pendientes...\")\n",
    "    pending_df = df.filter(col(\"solicitud_credito\").isNull())\n",
    "    pending_count = pending_df.count()\n",
    "    \n",
    "    if pending_count > 0:\n",
    "        print(f\"Procesando {pending_count} solicitudes pendientes...\")\n",
    "        predictions = clf_model.transform(pending_df)\n",
    "        \n",
    "        # Analizar predicciones\n",
    "        pred_distribution = predictions.groupBy(\"prediction\").count().toPandas()\n",
    "        print(\"\\nDistribución de predicciones:\")\n",
    "        print(pred_distribution)\n",
    "        \n",
    "        # Preparar resultados finales con tipos de datos estrictamente compatibles con SQL\n",
    "        final_predictions = predictions.select(\n",
    "            col(\"id_cliente\").cast(\"string\"),\n",
    "            col(\"estado_solicitud\").cast(\"string\"),\n",
    "            col(\"prediction\").cast(\"double\"),\n",
    "            when(col(\"prediction\") == 1.0, \"Aprobado\")\n",
    "            .otherwise(\"Rechazado\").alias(\"prediccion_aprobado\"),\n",
    "            get_prob_udf(\"probability\").alias(\"score_aprobacion\").cast(\"double\")\n",
    "        )\n",
    "        \n",
    "        # Validar y asegurar compatibilidad SQL\n",
    "        final_predictions = ensure_sql_compatibility(final_predictions, \"predicciones_pendientes\")\n",
    "        \n",
    "        # Verificar que no haya valores nulos\n",
    "        null_check = final_predictions.select([\n",
    "            count(when(col(c).isNull() | isnan(col(c)), c)).alias(c)\n",
    "            for c in final_predictions.columns\n",
    "        ]).first()\n",
    "        \n",
    "        null_columns = [col for col, count in null_check.asDict().items() if count > 0]\n",
    "        if null_columns:\n",
    "            raise ValueError(f\"Valores nulos encontrados en columnas: {', '.join(null_columns)}\")\n",
    "        \n",
    "        # Verificar rangos de valores\n",
    "        validation_checks = final_predictions.select(\n",
    "            (count(when((col(\"prediction\") != 0.0) & (col(\"prediction\") != 1.0), 1)) == 0).alias(\"valid_prediction\"),\n",
    "            (count(when((col(\"score_aprobacion\") < 0.0) | (col(\"score_aprobacion\") > 1.0), 1)) == 0).alias(\"valid_score\")\n",
    "        ).first()\n",
    "        \n",
    "        if not (validation_checks[\"valid_prediction\"] and validation_checks[\"valid_score\"]):\n",
    "            raise ValueError(\"Valores fuera de rango detectados en prediction o score_aprobacion\")\n",
    "        \n",
    "        # Intentar escribir con validación mejorada\n",
    "        success, error = write_delta_table_with_validation(final_predictions, lakehouse_path, \"predicciones_pendientes\")\n",
    "        if not success:\n",
    "            error_detail = f\"DeltaTableUserException: {error}\"\n",
    "            enviar_notificacion_tabla(\"predicciones_pendientes\", \"Error\", error_detail)\n",
    "            raise Exception(error_detail)\n",
    "        \n",
    "        # Verificar la tabla después de escribir\n",
    "        verify_success, verify_error = check_delta_table(lakehouse_path, \"predicciones_pendientes\")\n",
    "        if not verify_success:\n",
    "            error_detail = f\"Error de verificación post-escritura: {verify_error}\"\n",
    "            enviar_notificacion_tabla(\"predicciones_pendientes\", \"Error\", error_detail)\n",
    "            raise Exception(error_detail)\n",
    "        \n",
    "        avg_score = final_predictions.agg(avg(\"score_aprobacion\").alias(\"avg_score\")).collect()[0][\"avg_score\"]\n",
    "        \n",
    "        metrics = {\n",
    "            \"total_pendientes\": float(pending_count),\n",
    "            \"aprobados_predichos\": float(pred_distribution[pred_distribution[\"prediction\"] == 1.0][\"count\"].iloc[0]),\n",
    "            \"score_promedio\": float(avg_score)\n",
    "        }\n",
    "        \n",
    "        log_reentrenamiento(\"Clasificación\", \"Éxito\", metrics=metrics)\n",
    "        mensaje = crear_mensaje(\"Clasificación\", \"Éxito\", metrics=metrics)\n",
    "        enviar_notificacion_tabla(\"predicciones_pendientes\", \"Éxito\")\n",
    "    else:\n",
    "        mensaje = crear_mensaje(\"Clasificación\", \"Éxito\", error_message=\"No hay solicitudes pendientes\")\n",
    "        log_reentrenamiento(\"Clasificación\", \"Éxito\", error_message=\"No hay solicitudes pendientes\")\n",
    "    \n",
    "    enviar_notificacion(mensaje)\n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    log_reentrenamiento(\"Clasificación\", \"Error\", error_message=error_msg)\n",
    "    mensaje = crear_mensaje(\"Clasificación\", \"Error\", error_message=error_msg)\n",
    "    enviar_notificacion(mensaje)\n",
    "    enviar_notificacion_tabla(\"predicciones_pendientes\", \"Error\", error_msg)\n",
    "    print(f\"Error detallado: {error_msg}\")\n",
    "    raise Exception(f\"Error al aplicar clasificación o escribir tabla: {error_msg}\")\n",
    "\n",
    "# Notificación final\n",
    "try:\n",
    "    mensaje = crear_mensaje(\n",
    "        \"Proceso Completo\",\n",
    "        \"Éxito\",\n",
    "        metrics={\n",
    "            \"total_registros\": float(total_records),\n",
    "            \"solicitudes_pendientes\": float(pending_count if 'pending_count' in locals() else 0)\n",
    "        }\n",
    "    )\n",
    "    enviar_notificacion(mensaje)\n",
    "except Exception as e:\n",
    "    print(f\"Error al enviar notificación final: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86d0de7",
   "metadata": {},
   "source": [
    "## Respuesta de aplicación automatizada de modelos en MS Fabric\n",
    "\n",
    "![Respuesta de Aplicacion de Modelos](../Imagenes/00_Respuesta%20de%20Aplicacion%20de%20Modelos.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
