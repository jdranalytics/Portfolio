{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentación de Clientes por Región y Categoría\n",
    "\n",
    "Este notebook realiza la segmentación de clientes (o regiones/categorías como proxy) utilizando K-Means y PCA, basándose en datos de ventas, descuentos, márgenes y características temporales desde Snowflake. Los componentes principales incluyen:\n",
    "\n",
    "- **Carga de datos** desde la vista `vw_ventas_ml`.\n",
    "- **Preprocesamiento**: Agregación de datos por región y categoría, manejo de valores nulos, y escalado.\n",
    "- **Reducción de dimensionalidad**: Uso de PCA para reducir la complejidad de los datos.\n",
    "- **Clustering**: Aplicación de K-Means para identificar segmentos.\n",
    "- **Visualización**: Gráficos interactivos con Plotly para mostrar los clústeres.\n",
    "- **Almacenamiento**: Guardado de resultados en Snowflake.\n",
    "\n",
    "**Objetivo**: Identificar grupos de clientes o combinaciones región-categoría con patrones de compra similares para estrategias personalizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importar Librerías y Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import snowflake.connector\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "\n",
    "# Configurar advertencias y logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler('customer_segmentation.log')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conectar a la Base de Datos (Snowflake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snowflake_connection():\n",
    "    try:\n",
    "        conn = snowflake.connector.connect(\n",
    "            user='TU_USUARIO',\n",
    "            password='TU_CONTRASEÑA',\n",
    "            account='TU_CUENTA',\n",
    "            warehouse='TU_WAREHOUSE',\n",
    "            database='BEBIDAS_PROJECT',\n",
    "            schema='BEBIDAS_ANALYTICS'\n",
    "        )\n",
    "        logging.info(\"Conexión a Snowflake exitosa\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error de conexión a Snowflake: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cargar y Preprocesar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    try:\n",
    "        conn = get_snowflake_connection()\n",
    "        query = \"SELECT * FROM vw_ventas_ml\"\n",
    "        df = pd.read_sql(query, conn)\n",
    "        conn.close()\n",
    "        if df.empty:\n",
    "            raise ValueError(\"El DataFrame está vacío\")\n",
    "        logging.info(f\"Datos cargados: {len(df)} registros\")\n",
    "\n",
    "        # Verificar las columnas disponibles\n",
    "        logging.info(f\"Columnas disponibles: {df.columns.tolist()}\")\n",
    "\n",
    "        # Detectar dinámicamente las columnas relevantes\n",
    "        region_col = next((col for col in df.columns if 'region' in col.lower()), 'nombre_region')\n",
    "        categoria_col = next((col for col in df.columns if 'categoria' in col.lower()), 'categoria')\n",
    "        quantity_col = next((col for col in df.columns if any(keyword in col.lower() for keyword in ['cantidad', 'vendida', 'venta', 'unidades'])), 'CANTIDADES_VENDIDAS')\n",
    "        discount_col = next((col for col in df.columns if 'desc' in col.lower()), 'DESC_PORCENTAJE')\n",
    "        margin_col = next((col for col in df.columns if 'margen' in col.lower()), 'MARGEN_GANANCIA_BRUTA_PORCENTAJE')\n",
    "        price_col = next((col for col in df.columns if 'precio' in col.lower()), 'PRECIO_PROMEDIO')\n",
    "        logging.info(f\"Columnas detectadas: region={region_col}, categoria={categoria_col}, cantidad={quantity_col}, descuento={discount_col}, margen={margin_col}, precio={price_col}\")\n",
    "\n",
    "        # Agregar datos por región y categoría (proxy para clientes)\n",
    "        agg_df = df.groupby([region_col, categoria_col]).agg({\n",
    "            quantity_col: 'mean',\n",
    "            discount_col: 'mean',\n",
    "            margin_col: 'mean',\n",
    "            price_col: 'mean',\n",
    "            'MES': 'nunique'  # Número de meses con datos\n",
    "        }).reset_index()\n",
    "        agg_df = agg_df.rename(columns={quantity_col: 'avg_quantity', discount_col: 'avg_discount', margin_col: 'avg_margin', price_col: 'avg_price', 'MES': 'months_active'})\n",
    "\n",
    "        # Manejo de valores nulos\n",
    "        agg_df[['avg_quantity', 'avg_discount', 'avg_margin', 'avg_price', 'months_active']] = agg_df[['avg_quantity', 'avg_discount', 'avg_margin', 'avg_price', 'months_active']].fillna(0)\n",
    "\n",
    "        # Escalar los datos\n",
    "        scaler = StandardScaler()\n",
    "        features = ['avg_quantity', 'avg_discount', 'avg_margin', 'avg_price', 'months_active']\n",
    "        X_scaled = scaler.fit_transform(agg_df[features])\n",
    "\n",
    "        return agg_df, features, scaler, X_scaled, region_col, categoria_col\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en carga y preprocesamiento: {e}\")\n",
    "        raise\n",
    "\n",
    "agg_df, features, scaler, X_scaled, region_col, categoria_col = load_and_preprocess_data()\n",
    "print(f\"Datos preprocesados. Filas: {len(agg_df)}, Features: {features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reducción de Dimensionalidad con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(X_scaled):\n",
    "    try:\n",
    "        pca = PCA(n_components=2)  # Reducir a 2 componentes para visualización\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        explained_variance = pca.explained_variance_ratio_\n",
    "        logging.info(f\"Varianza explicada por componente: {explained_variance}\")\n",
    "        return X_pca, pca\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en PCA: {e}\")\n",
    "        raise\n",
    "\n",
    "X_pca, pca = apply_pca(X_scaled)\n",
    "agg_df = agg_df.assign(PC1=X_pca[:, 0], PC2=X_pca[:, 1])\n",
    "print(f\"PCA aplicada. Componentes: PC1, PC2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clustering con K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kmeans(X_pca):\n",
    "    try:\n",
    "        # Probar diferentes valores de k con el método del codo\n",
    "        inertia = []\n",
    "        k_range = range(1, 11)\n",
    "        for k in k_range:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "            kmeans.fit(X_pca)\n",
    "            inertia.append(kmeans.inertia_)\n",
    "\n",
    "        # Seleccionar k óptimo (por ejemplo, k=3 basado en el codo)\n",
    "        k_optimal = 3\n",
    "        kmeans = KMeans(n_clusters=k_optimal, random_state=42)\n",
    "        clusters = kmeans.fit_predict(X_pca)\n",
    "        logging.info(f\"Número óptimo de clústeres: {k_optimal}, Inercia: {kmeans.inertia_}\")\n",
    "\n",
    "        return clusters, kmeans\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en K-Means: {e}\")\n",
    "        raise\n",
    "\n",
    "clusters, kmeans = apply_kmeans(X_pca)\n",
    "agg_df = agg_df.assign(Cluster=clusters)\n",
    "print(f\"Clústeres asignados: {np.unique(clusters)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualización de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(agg_df, region_col, categoria_col):\n",
    "    try:\n",
    "        fig = px.scatter(agg_df, x='PC1', y='PC2', color='Cluster',\n",
    "                         hover_data=[region_col, categoria_col, 'avg_quantity', 'avg_discount'],\n",
    "                         title='Segmentación de Clientes por Región y Categoría')\n",
    "        fig.update_traces(marker=dict(size=12))\n",
    "        fig.show()\n",
    "\n",
    "        # Resumen por clúster\n",
    "        cluster_summary = agg_df.groupby('Cluster').agg({\n",
    "            'avg_quantity': 'mean',\n",
    "            'avg_discount': 'mean',\n",
    "            'avg_margin': 'mean',\n",
    "            'avg_price': 'mean',\n",
    "            'months_active': 'mean',\n",
    "            region_col: lambda x: x.mode()[0],  # Región más común\n",
    "            categoria_col: lambda x: x.mode()[0]  # Categoría más común\n",
    "        }).reset_index()\n",
    "        logging.info(\"Resumen por clúster:\")\n",
    "        logging.info(cluster_summary.to_string(index=False))\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en visualización: {e}\")\n",
    "        raise\n",
    "\n",
    "plot_clusters(agg_df, region_col, categoria_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Almacenar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(agg_df):\n",
    "    try:\n",
    "        timestamp = datetime(2025, 6, 29, 9, 32)  # Fecha y hora actuales\n",
    "        conn = get_snowflake_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Crear tabla en Snowflake\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS Customer_Segmentation (\n",
    "                id INTEGER AUTOINCREMENT START 1 INCREMENT 1,\n",
    "                timestamp TIMESTAMP_NTZ,\n",
    "                region VARCHAR(100),\n",
    "                categoria VARCHAR(100),\n",
    "                avg_quantity FLOAT,\n",
    "                avg_discount FLOAT,\n",
    "                avg_margin FLOAT,\n",
    "                avg_price FLOAT,\n",
    "                months_active INTEGER,\n",
    "                cluster INTEGER,\n",
    "                pc1 FLOAT,\n",
    "                pc2 FLOAT\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        # Truncar tabla\n",
    "        cursor.execute(\"TRUNCATE TABLE Customer_Segmentation\")\n",
    "\n",
    "        # Guardar resultados\n",
    "        results_file = 'customer_segmentation.csv'\n",
    "        agg_df[['region', 'categoria', 'avg_quantity', 'avg_discount', 'avg_margin', 'avg_price', 'months_active', 'Cluster', 'PC1', 'PC2']].to_csv(\n",
    "            results_file, index=False, header=['region', 'categoria', 'avg_quantity', 'avg_discount', 'avg_margin', 'avg_price', 'months_active', 'cluster', 'pc1', 'pc2']\n",
    "        )\n",
    "\n",
    "        cursor.execute(\n",
    "            f\"PUT file://{os.path.abspath(results_file)} @%{conn.database}.BEBIDAS_ANALYTICS.Customer_Segmentation AUTO_COMPRESS=TRUE\"\n",
    "        )\n",
    "        cursor.execute(\n",
    "            f\"COPY INTO Customer_Segmentation (timestamp, region, categoria, avg_quantity, avg_discount, avg_margin, avg_price, months_active, cluster, pc1, pc2)\"\n",
    "            f\" FROM @%{conn.database}.BEBIDAS_ANALYTICS.Customer_Segmentation/{os.path.basename(results_file)}\"\n",
    "            f\" FILE_FORMAT = (TYPE = 'CSV' SKIP_HEADER = 1 FIELD_DELIMITER = ',' TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS')\"\n",
    "            f\" ON_ERROR = 'CONTINUE'\"\n",
    "        )\n",
    "\n",
    "        conn.commit()\n",
    "        logging.info(\"Resultados de segmentación guardados exitosamente en Snowflake\")\n",
    "\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM Customer_Segmentation\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"Registros en Customer_Segmentation: {count}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        logging.error(f\"Error al guardar resultados en Snowflake: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "save_results(agg_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}