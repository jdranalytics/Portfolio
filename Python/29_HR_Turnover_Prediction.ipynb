{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a94d3823",
   "metadata": {},
   "source": [
    "# Predicción de Rotación de Personal (HR Turnover Prediction)\n",
    "\n",
    "Este notebook analiza datos de empleados para predecir el riesgo de rotación utilizando un modelo de Random Forest. El análisis incluye:\n",
    "- Carga de datos desde SQL Server\n",
    "- Preprocesamiento de datos\n",
    "- Entrenamiento del modelo\n",
    "- Evaluación de resultados\n",
    "- Almacenamiento de predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa45bd3",
   "metadata": {},
   "source": [
    "## 1. Importación de Bibliotecas\n",
    "\n",
    "Importamos las bibliotecas necesarias para el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eccdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymssql\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "import datetime\n",
    "import logging\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFE\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Configuración de logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402c133b",
   "metadata": {},
   "source": [
    "## 2. Conexión a Base de Datos y Consulta\n",
    "\n",
    "Configuramos la conexión a SQL Server y ejecutamos la consulta para obtener los datos de empleados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5ef484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de SQL Server\n",
    "SQL_SERVER = \"172.28.192.1:50121\\SQLEXPRESS\"\n",
    "SQL_DB = \"HR_Analytics\"\n",
    "SQL_USER = \"sa\"\n",
    "SQL_PASSWORD = \"123456\"\n",
    "\n",
    "# Conectar a SQL Server usando pymssql\n",
    "server_name = SQL_SERVER.split('\\\\')[0]  # Separar el nombre del servidor de la instancia\n",
    "conn = pymssql.connect(\n",
    "    server=server_name,\n",
    "    user=SQL_USER,\n",
    "    password=SQL_PASSWORD,\n",
    "    database=SQL_DB\n",
    ")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT w.employee_id, w.department, w.salary, w.hire_date, \n",
    "       AVG(k.hours_worked) as avg_hours_worked, \n",
    "       AVG(k.overtime_hours) as avg_overtime_hours, \n",
    "       AVG(s.satisfaction_score) as avg_satisfaction_score,\n",
    "       CASE WHEN w.status = 'Terminated' THEN 1 ELSE 0 END AS turnover\n",
    "FROM Workday_Employees w\n",
    "LEFT JOIN Kronos_TimeEntries k ON w.employee_id = k.employee_id\n",
    "LEFT JOIN Employee_Surveys s ON w.employee_id = s.employee_id\n",
    "GROUP BY w.employee_id, w.department, w.salary, w.hire_date, w.status\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "print(f\"Datos cargados: {len(df)} registros\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac1cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### QUALITY REPORT (PRE CLEANING)\n",
    "\n",
    "def create_data_quality_report(df):\n",
    "    # Initialize dictionary to store quality metrics\n",
    "    quality_report = {\n",
    "        'Column_Name': [],\n",
    "        'Data_Type': [],\n",
    "        'Total_Rows': [],\n",
    "        'Missing_Values': [],\n",
    "        'Missing_Percentage': [],\n",
    "        'Blank_Values': [],\n",
    "        'Blank_Percentage': [],\n",
    "        'Zero_Values': [],\n",
    "        'Zero_Percentage': [],\n",
    "        'Unique_Values': []\n",
    "    }\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    \n",
    "    for column in df.columns:\n",
    "        # Count missing values\n",
    "        missing_count = df[column].isna().sum()\n",
    "        \n",
    "        # Count blank values (empty strings)\n",
    "        blank_count = df[column].astype(str).str.strip().eq('').sum()\n",
    "        \n",
    "        # Count zero values for numeric columns\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            zero_count = (df[column] == 0).sum()\n",
    "        else:\n",
    "            zero_count = 0\n",
    "            \n",
    "        # Count unique values\n",
    "        unique_count = df[column].nunique()\n",
    "        \n",
    "        # Add to report\n",
    "        quality_report['Column_Name'].append(column)\n",
    "        quality_report['Data_Type'].append(str(df[column].dtype))\n",
    "        quality_report['Total_Rows'].append(total_rows)\n",
    "        quality_report['Missing_Values'].append(missing_count)\n",
    "        quality_report['Missing_Percentage'].append(round(missing_count/total_rows * 100, 2))\n",
    "        quality_report['Blank_Values'].append(blank_count)\n",
    "        quality_report['Blank_Percentage'].append(round(blank_count/total_rows * 100, 2))\n",
    "        quality_report['Zero_Values'].append(zero_count)\n",
    "        quality_report['Zero_Percentage'].append(round(zero_count/total_rows * 100, 2))\n",
    "        quality_report['Unique_Values'].append(unique_count)\n",
    "    \n",
    "    # Create DataFrame from the quality report\n",
    "    quality_df = pd.DataFrame(quality_report)\n",
    "    \n",
    "    # Sort by missing percentage (descending)\n",
    "    quality_df = quality_df.sort_values('Missing_Percentage', ascending=False)\n",
    "    \n",
    "    return quality_df\n",
    "\n",
    "try:\n",
    "    # Verify if df exists and is a DataFrame\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise NameError(\"df is not a valid DataFrame\")\n",
    "    \n",
    "    # Create the quality report\n",
    "    quality_report = create_data_quality_report(df)\n",
    "\n",
    "    selected_columns = df.columns.tolist()\n",
    "    Q_Report = quality_report[quality_report['Column_Name'].isin(selected_columns)]\n",
    "except NameError:\n",
    "    print(\"Error: Please make sure to run the previous cell that creates the 'df' DataFrame first.\")\n",
    "    print(\"The DataFrame should be created in the cell that executes the SQL query.\")\n",
    "\n",
    "Q_Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81951b8",
   "metadata": {},
   "source": [
    "## 3. Preprocesamiento de Datos\n",
    "\n",
    "Preparamos los datos para el entrenamiento del modelo:\n",
    "- Calculamos la antigüedad (tenure)\n",
    "- Codificamos variables categóricas\n",
    "- Tratamos valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c570472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular antigüedad\n",
    "df['tenure'] = (pd.to_datetime('today') - pd.to_datetime(df['hire_date'])).dt.days / 365\n",
    "\n",
    "# Manejo específico de avg_overtime_hours\n",
    "df['avg_overtime_hours'] = df['avg_overtime_hours'].fillna(0)\n",
    "\n",
    "# Manejo condicional de avg_hours_worked\n",
    "df.loc[(df['avg_hours_worked'].isna()) & (df['tenure'] > 1), 'avg_hours_worked'] = 8\n",
    "df.loc[(df['avg_hours_worked'].isna()) & (df['tenure'] <= 1), 'avg_hours_worked'] = 0\n",
    "\n",
    "# Imputación de avg_satisfaction_score usando XGBoost\n",
    "def impute_satisfaction_scores(df):\n",
    "    # Separar datos para entrenamiento (registros con satisfaction_score)\n",
    "    train_mask = ~df['avg_satisfaction_score'].isna()\n",
    "    features = ['tenure', 'avg_hours_worked', 'avg_overtime_hours', 'salary']\n",
    "    \n",
    "    X_train = df[train_mask][features]\n",
    "    y_train = df[train_mask]['avg_satisfaction_score']\n",
    "    \n",
    "    # Preparar datos para predicción\n",
    "    X_predict = df[~train_mask][features]\n",
    "    \n",
    "    # Crear y entrenar modelo XGBoost para imputación\n",
    "    imputer = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    imputer.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluar precisión del modelo\n",
    "    train_predictions = imputer.predict(X_train)\n",
    "    mse = mean_squared_error(y_train, train_predictions)\n",
    "    r2 = r2_score(y_train, train_predictions)\n",
    "    \n",
    "    print(\"\\nMétricas del modelo de imputación:\")\n",
    "    print(f\"Error cuadrático medio (MSE): {mse:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    # Realizar predicciones para valores faltantes\n",
    "    predictions = imputer.predict(X_predict)\n",
    "    \n",
    "    # Asignar predicciones a los valores faltantes\n",
    "    df.loc[~train_mask, 'avg_satisfaction_score'] = predictions\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Aplicar imputación\n",
    "df = impute_satisfaction_scores(df)\n",
    "\n",
    "# Codificar variables categóricas\n",
    "df = pd.get_dummies(df, columns=['department'], drop_first=True)\n",
    "\n",
    "print(\"\\nEstadísticas descriptivas después del preprocesamiento:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Verificar valores faltantes restantes\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nValores faltantes restantes:\")\n",
    "print(missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa08a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "### QUALITY REPORT (POST CLEANING)\n",
    "\n",
    "def create_data_quality_report(df):\n",
    "    # Initialize dictionary to store quality metrics\n",
    "    quality_report = {\n",
    "        'Column_Name': [],\n",
    "        'Data_Type': [],\n",
    "        'Total_Rows': [],\n",
    "        'Missing_Values': [],\n",
    "        'Missing_Percentage': [],\n",
    "        'Blank_Values': [],\n",
    "        'Blank_Percentage': [],\n",
    "        'Zero_Values': [],\n",
    "        'Zero_Percentage': [],\n",
    "        'Unique_Values': []\n",
    "    }\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    \n",
    "    for column in df.columns:\n",
    "        # Count missing values\n",
    "        missing_count = df[column].isna().sum()\n",
    "        \n",
    "        # Count blank values (empty strings)\n",
    "        blank_count = df[column].astype(str).str.strip().eq('').sum()\n",
    "        \n",
    "        # Count zero values for numeric columns\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            zero_count = (df[column] == 0).sum()\n",
    "        else:\n",
    "            zero_count = 0\n",
    "            \n",
    "        # Count unique values\n",
    "        unique_count = df[column].nunique()\n",
    "        \n",
    "        # Add to report\n",
    "        quality_report['Column_Name'].append(column)\n",
    "        quality_report['Data_Type'].append(str(df[column].dtype))\n",
    "        quality_report['Total_Rows'].append(total_rows)\n",
    "        quality_report['Missing_Values'].append(missing_count)\n",
    "        quality_report['Missing_Percentage'].append(round(missing_count/total_rows * 100, 2))\n",
    "        quality_report['Blank_Values'].append(blank_count)\n",
    "        quality_report['Blank_Percentage'].append(round(blank_count/total_rows * 100, 2))\n",
    "        quality_report['Zero_Values'].append(zero_count)\n",
    "        quality_report['Zero_Percentage'].append(round(zero_count/total_rows * 100, 2))\n",
    "        quality_report['Unique_Values'].append(unique_count)\n",
    "    \n",
    "    # Create DataFrame from the quality report\n",
    "    quality_df = pd.DataFrame(quality_report)\n",
    "    \n",
    "    # Sort by missing percentage (descending)\n",
    "    quality_df = quality_df.sort_values('Missing_Percentage', ascending=False)\n",
    "    \n",
    "    return quality_df\n",
    "\n",
    "try:\n",
    "    # Verify if df exists and is a DataFrame\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise NameError(\"df is not a valid DataFrame\")\n",
    "    \n",
    "    # Create the quality report\n",
    "    quality_report = create_data_quality_report(df)\n",
    "\n",
    "    selected_columns = df.columns.tolist()\n",
    "    Q_Report = quality_report[quality_report['Column_Name'].isin(selected_columns)]\n",
    "except NameError:\n",
    "    print(\"Error: Please make sure to run the previous cell that creates the 'df' DataFrame first.\")\n",
    "    print(\"The DataFrame should be created in the cell that executes the SQL query.\")\n",
    "\n",
    "Q_Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18136488",
   "metadata": {},
   "source": [
    "## 4.1 Selección de Features con RFE y XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63403761",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.1 Selección de Features con RFE y XGBoost\n",
    "\n",
    "# Preparar datos para RFE\n",
    "# Identificar columnas numéricas y dummy de departamentos\n",
    "numeric_features = ['salary', 'tenure', 'avg_hours_worked', 'avg_overtime_hours', 'avg_satisfaction_score']\n",
    "department_features = [col for col in df.columns if col.startswith('department_')]\n",
    "all_features = numeric_features + department_features\n",
    "\n",
    "print(\"Features totales para RFE:\")\n",
    "print(all_features)\n",
    "\n",
    "X_rfe = df[all_features]\n",
    "y_rfe = df['turnover']\n",
    "\n",
    "# Escalar solo las variables numéricas\n",
    "scaler = StandardScaler()\n",
    "X_rfe[numeric_features] = scaler.fit_transform(X_rfe[numeric_features])\n",
    "\n",
    "# Crear modelo XGBoost para RFE\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Aplicar RFE con más features a seleccionar para incluir departamentos\n",
    "rfe_selector = RFE(\n",
    "    estimator=xgb_model,\n",
    "    n_features_to_select=8,  # Aumentamos el número de features a seleccionar\n",
    "    step=1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rfe_selector.fit(X_rfe, y_rfe)\n",
    "\n",
    "# Mostrar resultados de RFE\n",
    "feature_ranking = pd.DataFrame({\n",
    "    'Feature': all_features,\n",
    "    'Selected': rfe_selector.support_,\n",
    "    'Ranking': rfe_selector.ranking_\n",
    "}).sort_values('Ranking')\n",
    "\n",
    "print(\"\\nResultados de la selección RFE:\")\n",
    "print(feature_ranking)\n",
    "\n",
    "# Separar features seleccionados por tipo\n",
    "selected_features = feature_ranking[feature_ranking['Selected']]['Feature'].tolist()\n",
    "selected_numeric = [f for f in selected_features if f in numeric_features]\n",
    "selected_departments = [f for f in selected_features if f in department_features]\n",
    "\n",
    "print(\"\\nFeatures numéricos seleccionados:\")\n",
    "print(selected_numeric)\n",
    "print(\"\\nDepartamentos seleccionados:\")\n",
    "print(selected_departments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25d425d",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento del Modelo\n",
    "\n",
    "Entrenamos el modelo usando los features seleccionados por RFE y datos escalados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af16560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar features seleccionados por RFE\n",
    "X = df[selected_features]\n",
    "y = df['turnover']\n",
    "\n",
    "# Dividir datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escalar features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Aplicar SMOTE para balancear las clases\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Definir parámetros para Grid Search con XGBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'min_child_weight': [1, 3],\n",
    "    'gamma': [0, 0.1]\n",
    "}\n",
    "\n",
    "# Crear modelo base XGBoost\n",
    "xgb_base = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='auc',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Realizar Grid Search con validación cruzada\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_base,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Entrenar modelo con Grid Search\n",
    "print(\"Iniciando búsqueda de mejores hiperparámetros...\")\n",
    "grid_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Imprimir mejores parámetros\n",
    "print(\"\\nMejores parámetros encontrados:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Usar directamente el mejor modelo del GridSearch\n",
    "model = grid_search.best_estimator_\n",
    "\n",
    "# Entrenar el modelo final\n",
    "model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calcular y mostrar métricas detalladas\n",
    "print(\"\\nReporte de clasificación detallado:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Mostrar matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Visualizar matriz de confusión\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.ylabel('Verdaderos')\n",
    "plt.xlabel('Predichos')\n",
    "plt.show()\n",
    "\n",
    "# Mostrar importancia de características\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Visualizar importancia de características\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "plt.title('Importancia de Características')\n",
    "plt.xlabel('Importancia')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calcular probabilidades\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Encontrar el mejor umbral de decisión\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_proba[:, 1] >= threshold).astype(int)\n",
    "    f1 = f1_score(y_test, y_pred_threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\nMejor umbral de decisión: {best_threshold:.2f}\")\n",
    "print(f\"F1-Score con mejor umbral: {best_f1:.4f}\")\n",
    "\n",
    "# Actualizar predicciones con el mejor umbral\n",
    "final_predictions = (y_pred_proba[:, 1] >= best_threshold).astype(int)\n",
    "\n",
    "# Calcular métricas finales\n",
    "metrics = {\n",
    "    'accuracy': accuracy_score(y_test, final_predictions),\n",
    "    'precision': precision_score(y_test, final_predictions, zero_division=0),\n",
    "    'recall': recall_score(y_test, final_predictions, zero_division=0),\n",
    "    'f1_score': f1_score(y_test, final_predictions, zero_division=0)\n",
    "}\n",
    "\n",
    "print(\"\\nMétricas finales con umbral optimizado:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b742023",
   "metadata": {},
   "source": [
    "## 6. Guardar Modelo y Métricas\n",
    "\n",
    "Guardamos el modelo entrenado y almacenamos las métricas en la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb847487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear directorio si no existe\n",
    "os.makedirs('Modelos Entrenados', exist_ok=True)\n",
    "\n",
    "# Guardar modelo\n",
    "model_path = 'Modelos Entrenados/turnover_model.pkl'\n",
    "joblib.dump(model, model_path)\n",
    "print(f\"Modelo guardado en: {model_path}\")\n",
    "\n",
    "# Guardar métricas en SQL Server\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\n",
    "    \"\"\"\n",
    "    INSERT INTO ML_Model_Accuracy (model_name, run_date, accuracy, precision, recall, f1_score)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s)\n",
    "    \"\"\",\n",
    "    ('Turnover_Prediction', datetime.datetime.now(), metrics['accuracy'],\n",
    "     metrics['precision'], metrics['recall'], metrics['f1_score'])\n",
    ")\n",
    "conn.commit() # Added commit to save change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa58098",
   "metadata": {},
   "source": [
    "## 7. Guardar Predicciones\n",
    "\n",
    "Generamos y guardamos las probabilidades de rotación para todos los empleados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d61a708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular probabilidades y predicciones de rotación\n",
    "X_scaled = scaler.transform(X)  # Escalar datos antes de predecir\n",
    "df['turnover_probability'] = model.predict_proba(X_scaled)[:, 1]\n",
    "df['predicted_turnover'] = (df['turnover_probability'] >= best_threshold).astype(int)\n",
    "\n",
    "# Guardar predicciones en la base de datos\n",
    "cursor = conn.cursor()\n",
    "for _, row in df.iterrows():\n",
    "    # Primero eliminar registros existentes\n",
    "    cursor.execute(\n",
    "        \"DELETE FROM Turnover_Predictions WHERE employee_id = %s\",\n",
    "        (row['employee_id'],)\n",
    "    )\n",
    "    \n",
    "    # Luego insertar el nuevo registro\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO Turnover_Predictions \n",
    "        (employee_id, prediction_date, turnover_probability, predicted_turnover)\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "        \"\"\",\n",
    "        (row['employee_id'], datetime.datetime.now(), row['turnover_probability'], row['predicted_turnover'])\n",
    "    )\n",
    "\n",
    "conn.commit() # Added commit to save change\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1123be",
   "metadata": {},
   "source": [
    "## 8. Cerrar conexión\n",
    "\n",
    "Cerrando conexión con el backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d3f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "print(\"Predicciones guardadas exitosamente\")\n",
    "\n",
    "# Mostrar las 10 predicciones más altas\n",
    "print(\"\\nTop 10 empleados con mayor riesgo de rotación:\")\n",
    "high_risk = df[['employee_id', 'turnover_probability', 'predicted_turnover']].sort_values('turnover_probability', ascending=False).head(10)\n",
    "print(high_risk)\n",
    "\n",
    "# Mostrar distribución de predicciones\n",
    "print(\"\\nDistribución de predicciones de rotación:\")\n",
    "print(df['predicted_turnover'].value_counts(normalize=True).round(3) * 100, \"%\")\n",
    "\n",
    "# Cerrar conexión\n",
    "conn.close()\n",
    "print(\"\\nConexión a la base de datos cerrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb71cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
