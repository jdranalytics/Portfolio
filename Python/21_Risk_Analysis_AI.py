import streamlit as st
import pandas as pd
from google.cloud import bigquery
from google.oauth2 import service_account
import requests
import json
import plotly.express as px
import plotly.graph_objects as go
import plotly.io as pio
from datetime import datetime
import time
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
import logging
import traceback
import numpy as np
import os
import re
import atexit
import tempfile

# Configurar el logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# Configuraci√≥n de la aplicaci√≥n Streamlit
st.set_page_config(page_title="Risk Analysis", layout="wide", page_icon="üí∞")

# Configuraci√≥n inicial
PROJECT_ID = "adroit-terminus-450816-r9"
DATASET_ID = "solicitudes_credito"
CREDENTIALS_PATH = "C:/Users/joey_/Desktop/AIRFLOW/adroit-terminus-450816-r9-1b90cfcf6a76.json"
DISCORD_WEBHOOK_URL = "https://discordapp.com/api/webhooks/1354192765130375248/MF7bEPPlHnrzgYnJJ4iev7xTr0TrxVpqKw_MOVVIRseppELwK0hBM7VMZf8DQnVPpvh6"
MODEL_LIST = ["gemini-2.0-flash", "deepseek-chat", "deepseek-reasoner"]
GEMINI_API_KEY_PATH = "C:/Users/joey_/Desktop/AIRFLOW/API KEYS/gemini.txt"
DEEPSEEK_API_KEY_PATH = "C:/Users/joey_/Desktop/AIRFLOW/API KEYS/deepseek.txt"
DEEPSEEK_API_BASE = "https://api.deepseek.com/v1"

# L√≠mite configurable de filas para pasar al prompt
MAX_ROWS = 1000  # Puedes ajustar este valor seg√∫n tus necesidades

# Ruta del archivo CSV temporal
TEMP_CSV_PATH = os.path.join(tempfile.gettempdir(), "solicitudes_maestra_temp.csv")

# Autenticaci√≥n con BigQuery
logger.info("Autenticando con BigQuery...")
try:
    credentials = service_account.Credentials.from_service_account_file(CREDENTIALS_PATH)
    client = bigquery.Client(credentials=credentials, project=PROJECT_ID)
    logger.info("Autenticaci√≥n con BigQuery completada.")
except Exception as e:
    st.error(f"Error al autenticar con BigQuery: {e}")
    logger.error("Error al autenticar con BigQuery: %s", str(e))
    client = None

# Funci√≥n para enviar mensajes a Discord
def send_discord_message(message, success=True):
    timestamp = datetime.now().isoformat()
    color = 65280 if success else 16711680
    emoji = "‚úÖ" if success else "‚ùå"
    
    payload = {
        "embeds": [{
            "title": f"{emoji} Notificaci√≥n de Carga BigQuery",
            "description": message,
            "color": color,
            "fields": [{"name": "Timestamp", "value": timestamp, "inline": True}],
            "footer": {"text": "Sistema de Monitoreo ETL: Proyecto Solicitudes de Cr√©dito"}
        }]
    }
    
    try:
        requests.post(DISCORD_WEBHOOK_URL, json=payload)
        logger.info("Mensaje enviado a Discord: %s", message)
    except Exception as e:
        st.error(f"Error al enviar mensaje a Discord: {e}")
        logger.error("Error al enviar mensaje a Discord: %s", str(e))

# Funci√≥n para cargar datos de solicitudes_maestra directamente desde BigQuery
def load_solicitudes_maestra():
    if client is None:
        logger.error("No se puede cargar datos porque la autenticaci√≥n con BigQuery fall√≥.")
        return pd.DataFrame()
    
    query = f"SELECT * FROM `{PROJECT_ID}.{DATASET_ID}.solicitudes_maestra`"
    logger.info("Ejecutando consulta BigQuery: %s", query)
    try:
        df = client.query(query).to_dataframe()
        logger.info("Consulta ejecutada. Filas: %d, Columnas: %s", len(df), df.columns.tolist())
        
        if df.empty:
            logger.warning("No se encontraron datos en solicitudes_maestra")
            return pd.DataFrame()
        
        df.columns = df.columns.str.lower()
        logger.info("Columnas normalizadas: %s", df.columns.tolist())
        
        if 'id_cliente' not in df.columns:
            logger.warning("Columna 'id_cliente' no encontrada. Columnas disponibles: %s", df.columns.tolist())
            send_discord_message("Advertencia: Columna 'id_cliente' no encontrada", success=False)
        else:
            logger.info("Columna 'id_cliente' encontrada.")
        
        send_discord_message("Datos de solicitudes_maestra cargados con √©xito desde BigQuery", success=True)
        return df
    except Exception as e:
        logger.error("Error al cargar datos de solicitudes_maestra: %s\n%s", str(e), traceback.format_exc())
        send_discord_message(f"Error al cargar datos: {e}", success=False)
        return pd.DataFrame()

# Funci√≥n para cargar datos desde el CSV temporal
def load_from_temp_csv():
    try:
        if os.path.exists(TEMP_CSV_PATH):
            df = pd.read_csv(TEMP_CSV_PATH)
            logger.info("Datos cargados desde el CSV temporal: %s", TEMP_CSV_PATH)
            return df
        else:
            logger.warning("El archivo CSV temporal no existe: %s", TEMP_CSV_PATH)
            return pd.DataFrame()
    except Exception as e:
        logger.error("Error al cargar datos desde el CSV temporal: %s", str(e))
        return pd.DataFrame()

# Funci√≥n para eliminar el CSV temporal al cerrar
def cleanup_temp_csv():
    try:
        if os.path.exists(TEMP_CSV_PATH):
            os.remove(TEMP_CSV_PATH)
            logger.info("Archivo CSV temporal eliminado: %s", TEMP_CSV_PATH)
        else:
            logger.info("No se encontr√≥ el archivo CSV temporal para eliminar: %s", TEMP_CSV_PATH)
    except Exception as e:
        logger.error("Error al eliminar el archivo CSV temporal: %s", str(e))

# Registrar la funci√≥n de limpieza para que se ejecute al cerrar el script
atexit.register(cleanup_temp_csv)

# Cargar datos al iniciar el asistente y generar el CSV temporal
if "datos_maestra" not in st.session_state:
    with st.spinner("Cargando datos desde BigQuery y generando CSV temporal..."):
        logger.info("Cargando datos desde BigQuery...")
        df = load_solicitudes_maestra()
        if not df.empty:
            # Guardar los datos en un CSV temporal
            try:
                df.to_csv(TEMP_CSV_PATH, index=False)
                logger.info("Archivo CSV temporal generado: %s", TEMP_CSV_PATH)
            except Exception as e:
                logger.error("Error al generar el archivo CSV temporal: %s", str(e))
                st.error(f"Error al generar el archivo CSV temporal: {e}")
        else:
            logger.warning("No se encontraron datos para generar el CSV temporal.")
            st.warning("No se encontraron datos en BigQuery para generar el CSV temporal.")
        st.session_state.datos_maestra = df
        logger.info("Datos cargados. Estado: %s", "Vac√≠o" if st.session_state.datos_maestra.empty else "Con datos")

# Verificar si el CSV temporal est√° disponible
csv_available = os.path.exists(TEMP_CSV_PATH)

# No detener el script si los datos est√°n vac√≠os, solo mostrar una advertencia
if st.session_state.datos_maestra.empty:
    st.warning("No hay datos disponibles para analizar. Contin√∫a con una consulta si deseas intentar de nuevo.")
    logger.warning("No hay datos disponibles para analizar.")
else:
    # Convertir fecha_solicitud, inicio_mes e inicio_semana a datetime
    for col in ['fecha_solicitud', 'inicio_mes', 'inicio_semana']:
        if col in st.session_state.datos_maestra.columns:
            try:
                st.session_state.datos_maestra[col] = pd.to_datetime(st.session_state.datos_maestra[col])
                if col == 'fecha_solicitud':
                    min_date = st.session_state.datos_maestra['fecha_solicitud'].min()
                    max_date = st.session_state.datos_maestra['fecha_solicitud'].max()
                    logger.info("Rango de fechas disponible: desde %s hasta %s", min_date, max_date)
                    st.info(f"Rango de fechas disponible: desde {min_date} hasta {max_date}")
            except Exception as e:
                st.error(f"Error al convertir la columna '{col}' a datetime: {e}")
                logger.error("Error al convertir %s: %s", col, str(e))

    # Mostrar columnas disponibles
    logger.info("Columnas disponibles en los datos: %s", st.session_state.datos_maestra.columns.tolist())

    # Mostrar informaci√≥n de la tabla
    table_columns = st.session_state.datos_maestra.columns.tolist()
    st.info(f"Analizando datos desde BigQuery con las columnas: {', '.join(table_columns)}")

    # Mostrar una muestra de los datos
    st.write("Muestra de los datos cargados (primeras 5 filas):")
    st.dataframe(st.session_state.datos_maestra.head())

    # Resumen de datos
    data_summary_dict = {
        "columns": table_columns,
        "num_records": len(st.session_state.datos_maestra),
        "sample_data": st.session_state.datos_maestra.head(5).to_dict(orient="records"),
        "date_range": f"Desde {min_date.strftime('%Y-%m-%d')} hasta {max_date.strftime('%Y-%m-%d')}"
    }
    data_summary_str = (
        f"Columnas: {data_summary_dict['columns']}\n"
        f"N√∫mero de registros: {data_summary_dict['num_records']}\n"
        f"Rango de fechas: {data_summary_dict['date_range']}\n"
        f"Muestra de datos: {data_summary_dict['sample_data']}"
    )
    logger.info("data_summary (string): %s", data_summary_str)

# Definir SYSTEM_ROLE con reglas claras
SYSTEM_ROLE = """
Eres un cient√≠fico de datos y analista de riesgo crediticio experto. Tu objetivo es analizar los datos de solicitudes de cr√©dito proporcionados en un DataFrame y responder a consultas del usuario con propuestas claras, c√≥digo Python ejecutable, tablas, gr√°ficas, respuestas textuales, conclusiones relevantes y recomendaciones accionables. Usa razonamiento l√≥gico y sigue estas reglas estrictas:

- CR√çTICO: NO SIMULES DATOS, NO LOS INVENTES Y TAMPOCO ASUMAS. DEBES USAR EXCLUSIVAMENTE LOS DATOS PROPORCIONADOS EN EL DATAFRAME (df), y 'df' est√° definido como "df = pd.read_csv(TEMP_CSV_PATH)". NO SE HACEN IMPUTACIONES A COLUMNAS CON NULL.
- Responde siempre en espa√±ol, independientemente del idioma de la consulta del usuario.
- Antes de responder cualquier consulta, verifica las columnas disponibles en el DataFrame usando df.columns y aseg√∫rate de que las columnas necesarias est√©n presentes.
- Columnas disponibles en el DataFrame: id_cliente, edad, ingresos_anuales, puntaje_crediticio, historial_pagos, deuda_actual, antiguedad_laboral, estado_civil, numero_dependientes, tipo_empleo, fecha_solicitud, solicitud_credito, inicio_mes, inicio_semana, predicted_solicitud_credito, probabilidad_aprobacion, cluster, estado.
- Reglas estrictas para identificar m√©tricas de solicitudes:
  - Para determinar si una solicitud fue aprobada o rechazada:
    - Usa la columna 'solicitud_credito' libres de NaN.
    - Si solicitud_credito == 1, la solicitud fue aprobada.
    - Si solicitud_credito == 0, la solicitud fue rechazada.
    - Si solicitud_credito es NaN, la solicitud est√° pendiente y NO debe usarse para calcular m√©tricas de aprobaci√≥n/rechazo.
  - Para determinar si una solicitud fue evaluada:
    - Usa la columna 'estado'.
    - Si estado == 'Evaluada', la solicitud tiene una decisi√≥n real (solicitud_credito no es NaN).
    - Si estado == 'Pendiente (Predicha)', la solicitud no tiene decisi√≥n real (solicitud_credito es NaN o NULL, predicted_solicitud_credito no es NaN).
  - M√©tricas espec√≠ficas:
    - **CR√çTICO:** Antes de realizar cualquier operaci√≥n con columnas num√©ricas (como calcular m√≠nimos, m√°ximos, segmentaciones, tasas, etc.), elimina los valores NaN usando .dropna() o filtrando con .notna(). Ejemplo: df['columna'].dropna().min() para calcular el m√≠nimo, o df[df['columna'].notna()] para filtrar filas.
    - Solicitudes aprobadas: Filtra donde solicitud_credito == 1, elimina NaN con df['solicitud_credito'].notna() y cuenta las filas.
    - Solicitudes rechazadas: Filtra donde solicitud_credito == 0, elimina NaN con df['solicitud_credito'].notna() y cuenta las filas.
    - Solicitudes predichas aprobadas: Filtra donde predicted_solicitud_credito == 1, elimina NaN con df['predicted_solicitud_credito'].notna() y cuenta las filas.
    - Solicitudes predichas rechazada: Filtra donde predicted_solicitud_credito == 0, elimina NaN con df['predicted_solicitud_credito'].notna() y cuenta las filas.
    - Total solicitudes evaluadas: Filtra donde estado == 'Evaluada' y cuenta las filas.
    - Total solicitudes predichas: Filtra donde estado == 'Pendiente (Predicha)' y cuenta las filas.
    - N√∫mero de clientes √∫nicos: Conta los valores √∫nicos en la columna id_cliente.
    - TASA DE APROBACI√ìN: (N√∫mero de solicitudes aprobadas / Total de solicitudes evaluadas) * 100. Se deben eliminar los NaN en solicitud_credito antes de calcular la tasa, usando df[df['solicitud_credito'].notna()].
    - fecha_solicitud: Fecha de la solicitud (formato YYYY-MM-DD).
    - inicio_mes: Primer d√≠a del mes de la fecha_solicitud (formato YYYY-MM-DD).
    - inicio_semana: Primer d√≠a de la semana de la fecha_solicitud (formato YYYY-MM-DD).
  - Reglas adicionales:
    - Los NaN o NULL en solicitud_credito y predicted_solicitud_credito no se grafican.
    - Para columnas num√©ricas derivadas como tasa_aprobacion, redondea siempre a 1 decimal para consistencia.
- Reglas estrictas sobre valores nulos:
  - Las √∫nicas columnas que pueden tener valores nulos (NaN) son: 'solicitud_credito', 'predicted_solicitud_credito' y 'probabilidad_aprobacion'.
  - Todas las dem√°s columnas (como 'deuda_actual', 'historial_pagos', 'estado', etc.) NO deben tener valores nulos.
  - Antes de usar cualquier columna (excepto las tres mencionadas), verifica que no tenga valores nulos usando df['columna'].isna().any(). Si se encuentran valores nulos en una columna que no deber√≠a tenerlos, genera un DataFrame 'df_main' con un mensaje de error (por ejemplo, df_main = pd.DataFrame({'Error': ["Columna 'deuda_actual' contiene valores nulos, lo cual no est√° permitido"]})).
- Reglas estrictas para la generaci√≥n de c√≥digo:
  1. **Carga expl√≠cita del DataFrame desde el CSV temporal:**
     - Al inicio del c√≥digo, carga los datos desde el archivo CSV temporal usando `df = pd.read_csv(TEMP_CSV_PATH)` (definida en el script principal) y asigna el resultado a `df`. 'df = pd.read_csv(TEMP_CSV_PATH)' debe estar definida en el script principal, que contiene la ruta al archivo CSV temporal (`solicitudes_maestra_temp.csv`) en el directorio temporal del sistema.
     - **CR√çTICO:** NO redefinas la variable `TEMP_CSV_PATH` en el c√≥digo generado. `TEMP_CSV_PATH` ya est√° definida en el script principal y apunta a la ruta correcta: `solicitudes_maestra_temp.csv` en el directorio temporal del sistema.
     - **CR√çTICO:** NO redefinas la funci√≥n `df = pd.read_csv(TEMP_CSV_PATH)` en el c√≥digo generado. Usa la funci√≥n tal como est√° definida en el script principal, que no acepta argumentos y usa `TEMP_CSV_PATH` internamente.
     - Define una lista `required_columns` con las columnas necesarias para la consulta.
     - Despu√©s de cargar `df`, aplica un `dropna()` al subconjunto de columnas requeridas usando `df = df.dropna(subset=required_columns)`. Esto elimina filas donde cualquiera de las columnas requeridas tenga valores NaN.
     - Si el DataFrame est√° vac√≠o o no se puede cargar, genera un df_main con un mensaje de error.
  2. Antes de realizar cualquier operaci√≥n con una columna (como calcular m√≠nimos, m√°ximos, o segmentaciones), verifica que:
     - La columna existe en el DataFrame usando 'columna in df.columns'.
     - Si la columna NO es 'solicitud_credito', 'predicted_solicitud_credito' ni 'probabilidad_aprobacion', verifica que no tenga valores nulos usando df['columna'].isna().any(). Si hay valores nulos, genera un DataFrame 'df_main' con un mensaje de error.
     - Si la columna es num√©rica, aseg√∫rate de que los valores sean num√©ricos usando pd.to_numeric(df['columna'], errors='coerce') y verifica que no haya valores no num√©ricos que causen problemas.
  3. Si una columna no existe, tiene valores nulos (cuando no deber√≠a), o no cumple con los requisitos (por ejemplo, no es num√©rica para operaciones que lo requieren), genera un DataFrame 'df_main' con una columna 'Error' que describa el problema (por ejemplo, df_main = pd.DataFrame({'Error': ['Columna "deuda_actual" no encontrada']})).
  4. Si no puedes proceder con el an√°lisis debido a datos insuficientes, genera una gr√°fica vac√≠a con un t√≠tulo que indique el problema (por ejemplo, fig = px.bar(title='No hay datos suficientes para generar el gr√°fico')).
  5. CR√çTICO: Siempre genera un DataFrame principal llamado 'df_main'. Si el an√°lisis falla por cualquier motivo (falta de datos, errores en los c√°lculos, etc.), asigna a 'df_main' un DataFrame con un mensaje de error.
  6. No uses funciones de Streamlit (como st.write(), st.plotly_chart(), etc.) dentro del c√≥digo generado. Las gr√°ficas deben asignarse a una variable 'fig' y los DataFrames a 'df_main' para que Streamlit los renderice fuera del c√≥digo generado.
  7. No llames a funciones externas (como calculate_approve_rate_heatmap) a menos que est√©n definidas expl√≠citamente dentro del c√≥digo generado, excepto la funci√≥n `df = pd.read_csv(TEMP_CSV_PATH)`, que est√° definida en el script principal.
- Flujo de trabajo para consultas de generaci√≥n de c√≥digo (Etapa 1):
  1. Razona la solicitud paso a paso, identificando lo que se solicita, las m√©tricas requeridas, las columnas necesarias y las condiciones de filtrado.
  2. Verifica la disponibilidad de las columnas necesarias y los datos en el DataFrame usando df.columns y las reglas de validaci√≥n mencionadas.
  3. Si no puedes ejecutar la consulta (por falta de columnas, datos insuficientes, valores nulos no permitidos, o condiciones no cumplidas), ind√≠calo claramente, explica por qu√©, y genera un DataFrame 'df_main' con un mensaje de error.
  4. Si puedes ejecutar la consulta, propone una soluci√≥n clara para resolverla, explicando los pasos a seguir.
  5. Genera c√≥digo Python que implemente la soluci√≥n usando pandas para data wrangling y plotly.express para visualizaci√≥n. Aseg√∫rate de que las gr√°ficas sean compatibles con Streamlit (usa variables como 'fig' y no llames a .show() ni uses print() para mostrar resultados; los resultados deben asignarse a variables para que Streamlit los renderice con st.write() o st.dataframe()).
  6. Genera SOLO UN DataFrame principal (el m√°s importante para el an√°lisis) que contenga los resultados clave de las operaciones (como agregaciones, transformaciones, o datos usados para gr√°ficas). If the query asks for a DataFrame or values, generate that DataFrame as the main one. If the query asks for a plot (like a heatmap), generate the DataFrame that feeds the plot as the main one. Nombra este DataFrame 'df_main'.
  7. Si no se puede calcular la m√©trica solicitada, sugiere una m√©trica alternativa que s√≠ se pueda calcular con los datos disponibles y genera el c√≥digo correspondiente, incluyendo un DataFrame principal.
  8. Proporciona el c√≥digo ejecutable bajo una secci√≥n titulada "Implementaci√≥n (c√≥digo Python):" en tu respuesta, asegur√°ndote de que est√© correctamente indentado y sea sint√°cticamente correcto.
  9. CR√çTICO: Aseg√∫rate de que 'df_main' siempre est√© definido al final del c√≥digo, incluso si hay errores en el flujo. Si el an√°lisis falla, asigna a 'df_main' un DataFrame con un mensaje de error.
"""
logger.info("SYSTEM_ROLE actualizado para que el dataframe sea df = pd.read_csv(TEMP_CSV_PATH)")

# Definir USER_ROLE_TEMPLATE para la primera etapa (generaci√≥n de c√≥digo)
USER_ROLE_TEMPLATE_CODE = """
Analiza los datos de solicitudes de cr√©dito y responde a la siguiente consulta: {prompt}. 

Los datos est√°n disponibles en un archivo CSV temporal cargando los datos desde este archivo usando 'df = pd.read_csv(TEMP_CSV_PATH)', (sin pasar argumentos, ya que la funci√≥n usa `TEMP_CSV_PATH` internamente) y asigna el resultado a una variable `df`. **CR√çTICO:** NO redefinas `TEMP_CSV_PATH` en el c√≥digo generado; ya est√° definida en el script principal. Despu√©s de cargar `df`, aplica un `dropna()` al subconjunto de columnas requeridas (`required_columns`) usando `df = df.dropna(subset=required_columns)` para eliminar filas donde cualquiera de las columnas requeridas tenga valores NaN. Realiza el an√°lisis siguiendo el flujo de trabajo definido en el SYSTEM_ROLE para consultas de generaci√≥n de c√≥digo (Etapa 1), utilizando pandas para data wrangling y plotly.express para visualizaci√≥n.

Datos preprocesados (si aplica):
{preprocessed_data}

Devuelve un an√°lisis que incluya:
CR√çTICO: USA RAZONAMIENTO L√ìGICO PASO A PASO, NO INVENTES DATOS, NO SIMULES, NO ASUMAS. SOLO DEBES USAR LOS DATOS DISPONIBLES EN EL DATAFRAME (df).
1. Razona la solicitud paso a paso, identificando lo que se solicita, las m√©tricas requeridas, las columnas necesarias y las condiciones de filtrado. Sigue estrictamente las reglas del SYSTEM_ROLE para identificar m√©tricas como aprobaciones, rechazos y solicitudes evaluadas.
2. Define una lista `required_columns` con las columnas necesarias para la consulta y aplica el `dropna()` al subconjunto de esas columnas despu√©s de cargar `df`.
3. Verifica la disponibilidad de las columnas necesarias en el DataFrame 'df' usando df.columns y aseg√∫rate de que las columnas est√©n presentes antes de proceder. Sigue las reglas de validaci√≥n de datos del SYSTEM_ROLE, incluyendo la verificaci√≥n de valores nulos en columnas que no deber√≠an tenerlos.
4. Si no puedes ejecutar la consulta, ind√≠calo claramente, explica por qu√© y sugiere una m√©trica alternativa que s√≠ se pueda calcular con los datos disponibles.
5. If puedes ejecutar la consulta (o una alternativa), propone una soluci√≥n clara, genera y muestra el c√≥digo Python que implemente la soluci√≥n usando el DataFrame 'df'. Aseg√∫rate de que el c√≥digo sea compatible with Streamlit: no uses fig.show(), print(), ni funciones de Streamlit como st.plotly_chart() para mostrar resultados; asigna las gr√°ficas a variables como 'fig' y los DataFrames a 'df_main' para que Streamlit los renderice con st.plotly_chart() o st.dataframe().
6. CR√çTICO: Genera SOLO UN DataFrame principal (el m√°s importante para el an√°lisis) que contenga los resultados clave de las operaciones. Si la consulta pide un DataFrame o valores, genera ese DataFrame como el principal (n√≥mbralo 'df_main'). Si la consulta pide una gr√°fica (como un heatmap), genera el DataFrame que alimenta la gr√°fica como el principal (n√≥mbralo 'df_main'). Este DataFrame ser√° analizado en la Etapa 2.
7. TODOS LOS DATAFRAMES PRINCIPALES SE LLAMAN 'df_main' y los gr√°ficos con 'plot_' o 'fig'. Si generas una gr√°fica, aseg√∫rate de que 'df_main' contenga los datos exactos usados para esa gr√°fica y que la gr√°fica se asigne a 'fig'.
8. CR√çTICO: Aseg√∫rate de que 'df_main' siempre est√© definido al final del c√≥digo, incluso si hay errores en el flujo. Si el an√°lisis falla, asigna a 'df_main' un DataFrame con un mensaje de error.
9. Proporciona el c√≥digo ejecutable bajo una secci√≥n titulada "Implementaci√≥n (c√≥digo Python):" en tu respuesta, asegur√°ndote de que est√© correctamente indentado y sea sint√°cticamente correcto.
"""
logger.info("USER_ROLE_TEMPLATE_CODE actualizado para evitar redefinici√≥n de TEMP_CSV_PATH en el c√≥digo generado.")

# Definir USER_ROLE_TEMPLATE para la segunda etapa (an√°lisis de DataFrames)
USER_ROLE_TEMPLATE_ANALYSIS = """
Analiza el DataFrame principal resultante generado a partir de la siguiente consulta inicial: {initial_prompt}.

A continuaci√≥n, se proporciona un resumen estad√≠stico del DataFrame principal 'df_main', junto con una muestra de las primeras filas (si aplica):

**Resumen estad√≠stico del DataFrame 'df_main':**
{df_summary}

**Muestra de las primeras filas del DataFrame 'df_main' (hasta {max_rows} filas, solo si el DataFrame tiene 1000 filas o menos):**
{df_sample}

Realiza el an√°lisis siguiendo el flujo de trabajo definido en el SYSTEM_ROLE para consultas de an√°lisis de DataFrames (Etapa 2). Proporciona un an√°lisis detallado, conclusiones y recomendaciones basadas exclusivamente en los datos proporcionados en el prompt, en el contexto de la consulta inicial.

IMPORTANTE: NO EJECUTES C√ìDIGO PARA CARGAR DATOS. Usa los valores exactamente como se proporcionan en el resumen estad√≠stico y la muestra del prompt. NO SIMULES DATOS, NO LOS INVENTES Y TAMPOCO ASUMAS. DEBES USAR EXCLUSIVAMENTE LOS DATOS PROPORCIONADOS EN EL PROMPT.
CR√çTICO: DEBES MANTENER EL MISMO PER√çODO DE EVALUACI√ìN DE LA CONSULTA INICIAL.

- Si el DataFrame tiene m√°s de 1000 filas, realiza un an√°lisis basado √∫nicamente en el resumen estad√≠stico (estad√≠sticas descriptivas, conteos de valores categ√≥ricos y correlaciones), sin considerar la muestra de filas.
- Si el DataFrame tiene 1000 filas o menos, realiza un an√°lisis completo que incluya tanto el resumen estad√≠stico como la muestra de filas, explorando tendencias, patrones y relaciones.

Devuelve: 'Periodo de Evaluaci√≥n = Fecha inicial y fecha final de la consulta' tomando en cuenta hasta donde hay datos disponibles.
0. Usa los datos del resumen estad√≠stico y la muestra proporcionada (si aplica), sin recalcular m√©tricas. Genera una tabla de estad√≠sticas descriptivas de los datos sin tomar en cuenta 'id_cliente' y usando el periodo de evaluaci√≥n.
1. Un an√°lisis textual detallado de los datos con an√°lisis num√©rico, explorando columnas, valores, tendencias, m√°ximos, m√≠nimos, promedios, correlaciones, distribuciones, y patrones relevantes.
2. Conclusiones relevantes basadas en el an√°lisis haciendo menci√≥n a valores num√©ricos clave.
3. Recomendaciones accionables basadas en los datos y el contexto de la consulta, mencionando umbrales de valores num√©ricos clave y recomendaciones relevantes.
"""
logger.info("USER_ROLE_TEMPLATE_ANALYSIS actualizado para manejar DataFrames grandes y peque√±os.")

# Funciones auxiliares
def check_model_availability(model_name, api_key):
    try:
        if "gemini" in model_name:
            llm = ChatGoogleGenerativeAI(model=model_name, google_api_key=api_key, temperature=0.3)
            llm.invoke("Ping")
        elif "deepseek" in model_name:
            llm = ChatOpenAI(model=model_name, api_key=api_key, base_url=DEEPSEEK_API_BASE, temperature=0.3)
        logger.info("Modelo %s disponible.", model_name)
        return "üü¢"
    except Exception as e:
        logger.error("Modelo %s no disponible: %s", model_name, str(e))
        return "üî¥"

# Funci√≥n para generar un resumen estad√≠stico del DataFrame
def generate_dataframe_summary(df):
    if df.empty:
        return "El DataFrame est√° vac√≠o."
    
    summary = []
    summary.append(f"N√∫mero total de filas: {len(df)}")
    summary.append(f"Columnas: {', '.join(df.columns.tolist())}")
    
    # Estad√≠sticas descriptivas para columnas num√©ricas
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        desc = df[numeric_cols].describe().round(2).to_markdown(index=True)
        summary.append("\n**Estad√≠sticas descriptivas (columnas num√©ricas):**\n")
        summary.append(desc)
    
    # Conteos para columnas categ√≥ricas
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns
    for col in categorical_cols:
        value_counts = df[col].value_counts().head(5).to_markdown(index=True)
        summary.append(f"\n**Conteo de valores para la columna '{col}' (top 5):**\n")
        summary.append(value_counts)
    
    # Correlaciones entre columnas num√©ricas (si aplica)
    if len(numeric_cols) > 1:
        corr = df[numeric_cols].corr().round(2).to_markdown(index=True)
        summary.append("\n**Matriz de correlaciones (columnas num√©ricas):**\n")
        summary.append(corr)
    
    return "\n".join(summary)

# Funci√≥n para convertir un DataFrame a formato Markdown con l√≠mite de filas
def dataframe_to_markdown(df, max_rows=MAX_ROWS):
    if df.empty:
        return "El DataFrame est√° vac√≠o."
    
    # Limitar el n√∫mero de filas seg√∫n max_rows
    if len(df) > max_rows:
        df = df.head(max_rows)
        note = f"\n**Nota:** Se muestran solo las primeras {max_rows} filas de un total de {len(df)}."
    else:
        note = ""
    
    try:
        # Intentar convertir el DataFrame a Markdown usando tabulate
        markdown = df.to_markdown(index=False)
        return f"{markdown}{note}"
    except ImportError as e:
        # Si tabulate no est√° instalado, usar una representaci√≥n alternativa
        logger.warning("No se encontr√≥ 'tabulate'. Usando representaci√≥n alternativa para el DataFrame.")
        lines = ["| " + " | ".join(df.columns) + " |"]
        lines.append("| " + " | ".join(["---"] * len(df.columns)) + " |")
        for _, row in df.iterrows():
            lines.append("| " + " | ".join(str(val) for val in row) + " |")
        alternative = "\n".join(lines)
        return f"{alternative}{note}\n**Advertencia:** No se encontr√≥ 'tabulate'. Se recomienda instalarlo con 'pip install tabulate' para una mejor representaci√≥n."

# Funci√≥n para limpiar el c√≥digo extra√≠do (correcci√≥n b√°sica de indentaci√≥n y l√≠neas vac√≠as)
def clean_code(code_str):
    # Dividir el c√≥digo en l√≠neas
    lines = code_str.splitlines()
    # Eliminar l√≠neas vac√≠as y espacios innecesarios
    lines = [line.rstrip() for line in lines if line.strip()]
    
    # Detectar el nivel de indentaci√≥n base (asumiendo que la primera l√≠nea no est√° indentada)
    if not lines:
        return ""
    
    # Corregir indentaci√≥n b√°sica
    cleaned_lines = []
    for line in lines:
        # Contar los espacios al inicio de la l√≠nea
        leading_spaces = len(line) - len(line.lstrip())
        # Asegurar que la indentaci√≥n sea m√∫ltiplo de 4 (est√°ndar en Python)
        corrected_spaces = (leading_spaces // 4) * 4
        # Reemplazar la indentaci√≥n original con la corregida
        cleaned_line = " " * corrected_spaces + line.lstrip()
        cleaned_lines.append(cleaned_line)
    
    return "\n".join(cleaned_lines)

# Funci√≥n para eliminar bloques de c√≥digo de una respuesta para evitar duplicados en la visualizaci√≥n
def remove_code_blocks(response):
    # Usar una expresi√≥n regular para encontrar y eliminar bloques de c√≥digo (```python ... ```)
    return re.sub(r'```python\n.*?\n```', '[C√≥digo Python removido para evitar duplicaci√≥n]', response, flags=re.DOTALL)

# Funci√≥n para ejecutar c√≥digo Python generado y capturar resultados
def execute_generated_code(code_str):
    try:
        local_vars = {'pd': pd, 'px': px, 'go': go, 'np': np, 'load_from_temp_csv': load_from_temp_csv, 'TEMP_CSV_PATH': TEMP_CSV_PATH}
        exec(code_str, globals(), local_vars)
        dataframes = {k: v for k, v in local_vars.items() if isinstance(v, pd.DataFrame) and k != 'df'}
        plots = {k: v for k, v in local_vars.items() if isinstance(v, (go.Figure, dict)) and (k.startswith('plot_') or k == 'fig')}
        variables = {k: v for k, v in local_vars.items() if not isinstance(v, (pd.DataFrame, go.Figure, dict)) and k not in ['pd', 'px', 'go', 'df', 'np', 'load_from_temp_csv', 'TEMP_CSV_PATH']}
        
        # Asegurarse de que df_main siempre est√© presente
        if 'df_main' not in dataframes:
            if dataframes:
                # Si no hay df_main pero hay otros DataFrames, usar el √∫ltimo creado
                last_df_name = list(dataframes.keys())[-1]
                dataframes['df_main'] = dataframes[last_df_name]
                logger.info(f"No se encontr√≥ 'df_main'. Usando el √∫ltimo DataFrame creado: {last_df_name}")
            else:
                # Si no hay ning√∫n DataFrame, crear uno con un mensaje de error
                dataframes['df_main'] = pd.DataFrame({'Error': ['No se gener√≥ ning√∫n DataFrame en el c√≥digo']})
                if 'fig' not in plots:
                    plots['fig'] = px.bar(title="No se gener√≥ ning√∫n DataFrame ni gr√°fico")
                logger.warning("No se gener√≥ ning√∫n DataFrame en el c√≥digo. Creando df_main con mensaje de error.")
        
        return {
            'dataframes': dataframes,
            'plots': plots,
            'variables': variables
        }
    except NameError as ne:
        logger.error(f"Error de definici√≥n (NameError) al ejecutar el c√≥digo generado: {str(ne)}")
        error_df = pd.DataFrame({'Error': [f"Error de definici√≥n (NameError): {str(ne)}"]})
        error_fig = px.bar(title="Error de definici√≥n en el c√≥digo")
        return {
            'dataframes': {'df_main': error_df},
            'plots': {'fig': error_fig},
            'variables': {},
            'error': str(ne)
        }
    except Exception as e:
        logger.error(f"Error al ejecutar el c√≥digo generado: {str(e)}")
        error_df = pd.DataFrame({'Error': [f"Error en la ejecuci√≥n del c√≥digo: {str(e)}"]})
        error_fig = px.bar(title="Error en la ejecuci√≥n del c√≥digo")
        return {
            'dataframes': {'df_main': error_df},
            'plots': {'fig': error_fig},
            'variables': {},
            'error': str(e)
        }

# Funci√≥n para invocar el modelo de IA y obtener una respuesta
def invoke_model(prompt_template, variables):
    if llm is None:
        return "Error: No se ha configurado un modelo de IA v√°lido. Por favor, verifica la disponibilidad del modelo y las claves API."
    prompt = prompt_template.format(**variables)
    try:
        response = llm.invoke(prompt)
        return response.content
    except Exception as e:
        logger.error(f"Error al invocar el modelo: {str(e)}")
        return f"Error al invocar el modelo: {str(e)}"

# Inicializar claves API y disponibilidad de modelos
if "api_keys" not in st.session_state:
    st.session_state.api_keys = {}
    for model, path in [("gemini-2.0-flash", GEMINI_API_KEY_PATH), ("deepseek-chat", DEEPSEEK_API_KEY_PATH), 
                        ("deepseek-reasoner", DEEPSEEK_API_KEY_PATH)]:
        try:
            with open(path, "r") as file:
                st.session_state.api_keys[model] = file.read().strip()
        except Exception as e:
            st.session_state.api_keys[model] = None
            st.sidebar.error(f"Error al leer la clave API para {model}: {e}")

if "model_status" not in st.session_state:
    st.session_state.model_status = {model: check_model_availability(model, st.session_state.api_keys.get(model, "")) 
                                     if st.session_state.api_keys.get(model) else "üî¥" for model in MODEL_LIST}

# Barra lateral
st.sidebar.title("üë®‚Äçüíªüìä An√°lisis de Riesgo Crediticio")
st.sidebar.markdown("Consulta los datos de solicitudes de cr√©dito con un enfoque en riesgo crediticio.")

# Indicador de datos cargados
if "datos_maestra" in st.session_state and not st.session_state.datos_maestra.empty:
    st.sidebar.success("Datos cargados directamente desde BigQuery.")
else:
    st.sidebar.error("Datos no disponibles.")

# Indicador de disponibilidad del CSV temporal
if csv_available:
    st.sidebar.success("Archivo CSV temporal disponible: solicitudes_maestra_temp.csv")
else:
    st.sidebar.error("Archivo CSV temporal no disponible: solicitudes_maestra_temp.csv")

# Bot√≥n para limpiar cach√©
if "clear_cache_counter" not in st.session_state:
    st.session_state.clear_cache_counter = 0

clear_cache_key = f"clear_cache_button_{st.session_state.clear_cache_counter}"
if st.sidebar.button("Limpiar cach√©", key=clear_cache_key):
    st.session_state.clear_cache_counter += 1
    st.cache_data.clear()
    for key in list(st.session_state.keys()):
        if key not in ['api_keys', 'model_status', 'clear_cache_counter']:
            del st.session_state[key]
    st.session_state.datos_maestra = load_solicitudes_maestra()
    if not st.session_state.datos_maestra.empty:
        try:
            st.session_state.datos_maestra.to_csv(TEMP_CSV_PATH, index=False)
            logger.info("Archivo CSV temporal actualizado despu√©s de limpiar cach√©: %s", TEMP_CSV_PATH)
        except Exception as e:
            logger.error("Error al actualizar el archivo CSV temporal despu√©s de limpiar cach√©: %s", str(e))
    st.rerun()

# Mostrar disponibilidad de modelos
st.sidebar.subheader("Disponibilidad de Modelos")
for model, status in st.session_state.model_status.items():
    st.sidebar.write(f"{model}: {status}")

# Selecci√≥n del modelo
if "model_select_counter" not in st.session_state:
    st.session_state.model_select_counter = 0

model_select_key = f"model_select_{st.session_state.model_select_counter}"
model_option = st.sidebar.selectbox("Elige el modelo", MODEL_LIST, index=0, key=model_select_key)

# Configurar el modelo seleccionado
llm = None
if st.session_state.model_status[model_option] == "üü¢":
    try:
        if "gemini" in model_option:
            llm = ChatGoogleGenerativeAI(model=model_option, google_api_key=st.session_state.api_keys[model_option], 
                                         temperature=0.3)
        elif "deepseek" in model_option:
            llm = ChatOpenAI(model=model_option, api_key=st.session_state.api_keys[model_option], 
                             base_url=DEEPSEEK_API_BASE, temperature=0.3)
        st.sidebar.success(f"¬°Conexi√≥n con {model_option} establecida!")
    except Exception as e:
        st.sidebar.error(f"Error al conectar con {model_option}: {e}")
else:
    st.sidebar.error(f"El modelo {model_option} no est√° disponible. Contin√∫a con una consulta si deseas intentar de nuevo.")

# Mostrar ejemplos en el sidebar justo debajo del aviso del modelo seleccionado
st.sidebar.markdown("### üìù Ejemplos de consultas")
example_queries = {
    "üìà Tendencia de aprobaciones": "Necesito una gr√°fica de l√≠nea con la tendencia agrupados mes a mes para los √∫ltimos 12 meses, en donde pueda ver la tasa de aprobaciones de los casos evaluados (Porcentaje con 1 decimal).",
    "üìä An√°lisis por segmento": "Muestra un an√°lisis de la tasa de aprobaci√≥n por segmento de ingresos_anuales, agrupando en rangos de 10000 y ordenados de mayor a menor tasa.",
    "üîÑ Correlaci√≥n de variables": "Genera una matriz de correlaci√≥n entre las variables num√©ricas: ingresos_anuales, puntaje_crediticio, deuda_actual y antiguedad_laboral.",
    "‚öñÔ∏è Perfil de riesgo": "Analiza el perfil de riesgo de los clientes mostrando la distribuci√≥n del puntaje_crediticio y la tasa de aprobaci√≥n por cada rango de 50 puntos.",
    "üßÆ Heatmap": "Crea un heatmap que visualice la tasa de aprobaci√≥n (porcentaje o ratio) en funci√≥n de dos variables: Eje X: Deuda actual en d√≥lares (USD), dividida en 6 segmentos equidistantes entre el valor m√≠nimo y m√°ximo de la deuda. Categoriza los segmentos con rangos de valores (min-max). Eje Y: Ingresos anuales en d√≥lares (USD), divididos en 6 segmentos equidistantes entre el valor m√≠nimo y m√°ximo de los ingresos. Categoriza los segmentos con rangos de valores (min-max). El heatmap debe usar la paleta RdBu con etiquetas de datos visibles en porcentaje de un decimal. Ordena los segmentos del menor al mayor."
}

with st.sidebar.expander("üîç Ver ejemplos de consultas", expanded=False):
    st.info("Haga clic en cualquier ejemplo para usarlo como consulta:")
    for label, query in example_queries.items():
        if st.button(label, key=f"sidebar_example_{label}"):
            if "chat_input_counter" not in st.session_state:
                st.session_state.chat_input_counter = 0
            st.session_state.chat_input_counter += 1
            st.session_state.selected_query = query
            st.rerun()

# Inicializar historial de mensajes
msgs = StreamlitChatMessageHistory(key="langchain_messages")
if len(msgs.messages) == 0:
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    initial_message = f"[{timestamp}] ¬øC√≥mo puedo ayudarte con el an√°lisis de riesgo crediticio? / How can I assist you with credit risk analysis?"
    msgs.add_ai_message(initial_message)

if "plots" not in st.session_state:
    st.session_state.plots = []
if "dataframes" not in st.session_state:
    st.session_state.dataframes = []

# Funci√≥n para mostrar el historial de chat
def display_chat_history():
    for msg in msgs.messages:
        avatar = "üë®‚Äçüíª" if msg.type == "human" else "ü§ñ"
        with st.chat_message(msg.type, avatar=avatar):
            message_content = msg.content
            if message_content.startswith("["):
                timestamp_end = message_content.find("]")
                if timestamp_end != -1:
                    timestamp = message_content[1:timestamp_end]
                    content = message_content[timestamp_end + 2:]
                    st.markdown(f"**{timestamp}** {content}")
            else:
                st.write(message_content)
            if "PLOT_INDEX:" in message_content:
                plot_index = int(message_content.split("PLOT_INDEX:")[1])
                st.plotly_chart(st.session_state.plots[plot_index], key=f"history_plot_{plot_index}")
            elif "DATAFRAME_INDEX:" in message_content:
                df_index = int(message_content.split("DATAFRAME_INDEX:")[1])
                st.dataframe(st.session_state.dataframes[df_index], key=f"history_dataframe_{df_index}")

# Interfaz principal
st.title("An√°lisis de Riesgo Crediticio")
st.markdown("Consulta los datos de solicitudes de cr√©dito con un enfoque en riesgo crediticio.")

display_chat_history()

# Entrada de consulta
st.write("### Ingrese su consulta:")
if "chat_input_counter" not in st.session_state:
    st.session_state.chat_input_counter = 0

chat_input_key = f"chat_input_{st.session_state.chat_input_counter}"

# Si hay una consulta seleccionada, mostrarla como texto antes del chat input
if "selected_query" in st.session_state:
    st.info("üìù Ejemplo seleccionado (puede copiarlo y modificarlo):")
    st.code(st.session_state.selected_query)
    # Limpiar la consulta seleccionada despu√©s de mostrarla
    st.session_state.pop("selected_query", None)

prompt = st.chat_input(
    placeholder="Ej: Genera un heatmap de la tasa de aprobaci√≥n evaluada...", 
    key=chat_input_key
)

# Procesar la consulta
if prompt:
    st.session_state.chat_input_counter += 1  # Incrementar el contador para forzar un nuevo key en el pr√≥ximo renderizado
    logger.info(f"Prompt recibido: {prompt}")
    
    with st.spinner(f"Procesando tu consulta..."):
        try:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            user_message = f"[{timestamp}] {prompt}"
            st.chat_message("human", avatar="üë®‚Äçüíª").write(f"**{timestamp}** - {prompt}")
            msgs.add_user_message(user_message)
            logger.info("Consulta del usuario recibida: %s", prompt)

            # Etapa 1: Razonar y generar c√≥digo
            st.chat_message("ai", avatar="ü§ñ").write(f"**{timestamp}** - Razonamiento de la solicitud (Etapa 1):")
            variables = {
                "prompt": prompt,
                "preprocessed_data": data_summary_str
            }
            reasoning_response = invoke_model(USER_ROLE_TEMPLATE_CODE, variables)
            
            # Eliminar el bloque de c√≥digo del reasoning_response para evitar duplicaci√≥n
            cleaned_reasoning_response = remove_code_blocks(reasoning_response)
            st.write(cleaned_reasoning_response)

            # Extraer el c√≥digo de la secci√≥n "Implementaci√≥n (c√≥digo Python):"
            implementation_section = "Implementaci√≥n (c√≥digo Python):"
            code_start_marker = "```python"
            code_end_marker = "```"
            
            # Buscar la secci√≥n de implementaci√≥n
            implementation_start = reasoning_response.find(implementation_section)
            if implementation_start == -1:
                st.chat_message("ai", avatar="ü§ñ").write(f"**{timestamp}** - No se encontr√≥ la secci√≥n 'Implementaci√≥n (c√≥digo Python):' en la respuesta del modelo.")
                msgs.add_ai_message(f"[{timestamp}] No se encontr√≥ la secci√≥n 'Implementaci√≥n (c√≥digo Python):' en la respuesta del modelo.")
                st.stop()

            # Buscar el bloque de c√≥digo dentro de la secci√≥n de implementaci√≥n
            code_start = reasoning_response.find(code_start_marker, implementation_start)
            code_end = reasoning_response.find(code_end_marker, code_start + len(code_start_marker))
            if code_start == -1 or code_end == -1:
                st.chat_message("ai", avatar="ü§ñ").write(f"**{timestamp}** - No se pudo encontrar un bloque de c√≥digo Python v√°lido bajo la secci√≥n 'Implementaci√≥n (c√≥digo Python):'.")
                msgs.add_ai_message(f"[{timestamp}] No se pudo encontrar un bloque de c√≥digo Python v√°lido bajo la secci√≥n 'Implementaci√≥n (c√≥digo Python):'.")
                st.stop()

            # Extraer y limpiar el c√≥digo
            code_str = reasoning_response[code_start + len(code_start_marker):code_end].strip()
            code_str = clean_code(code_str)
            
            st.chat_message("ai", avatar="ü§ñ").write(f"**{timestamp}** - C√≥digo Python de implementaci√≥n:")
            st.code(code_str, language="python")

            # Ejecutar c√≥digo
            st.chat_message("ai", avatar="ü§ñ").write(f"**{timestamp}** - Ejecutando el c√≥digo de implementaci√≥n...")
            result = execute_generated_code(code_str)

            # Almacenar el DataFrame principal
            df_main = result['dataframes']['df_main']  # Ahora siempre estar√° presente gracias a execute_generated_code
            
            # Asegurar que columnas num√©ricas como tasa_aprobacion est√©n redondeadas a 1 decimal
            if 'tasa_aprobacion' in df_main.columns:
                df_main['tasa_aprobacion'] = df_main['tasa_aprobacion'].round(1)
            
            st.chat_message("ai", avatar="ü§ñ").write(f"**{timestamp}** - Tabla principal generada:")
            st.write("Tabla: df_main")
            st.dataframe(df_main)
            # Agregar opci√≥n para descargar el DataFrame como CSV
            csv = df_main.to_csv(index=False)
            st.download_button(
                label="Descargar df_main como CSV",
                data=csv,
                file_name="df_main.csv",
                mime="text/csv"
            )
            df_index = len(st.session_state.dataframes)
            st.session_state.dataframes.append(df_main)
            logger.info(f"DataFrame principal generado: df_main, filas: {len(df_main)}")
            logger.info(f"DataFrame generado en Etapa 1:\n{df_main.to_string()}")
            msgs.add_ai_message(f"[{timestamp}] DATAFRAME_INDEX:{df_index}")

            if result['plots']:
                st.chat_message("ai", avatar="ü§ñ").write(f"**{timestamp}** - Gr√°ficos generados:")
                for name, plot in result['plots'].items():
                    if isinstance(plot, dict):
                        plot = pio.from_json(json.dumps(plot))
                    st.plotly_chart(plot)
                    plot_index = len(st.session_state.plots)
                    st.session_state.plots.append(plot)
                    logger.info(f"Gr√°fico generado: {name}")
                    msgs.add_ai_message(f"[{timestamp}] PLOT_INDEX:{plot_index}")

            # Generar un resumen estad√≠stico del DataFrame
            df_summary = generate_dataframe_summary(df_main)
            
            # Determinar si incluir la muestra de filas (solo si el DataFrame tiene 1000 filas o menos)
            if len(df_main) <= 1000:
                df_sample = dataframe_to_markdown(df_main, max_rows=MAX_ROWS)
            else:
                df_sample = "No se incluye muestra de filas porque el DataFrame tiene m√°s de 1000 filas."

            # Etapa 2: An√°lisis del DataFrame principal pasado como texto en el prompt
            st.chat_message("ai", avatar="ü§ñ").write(f"**{timestamp}** - An√°lisis del DataFrame principal (Etapa 2):")
            analysis_prompt = {
                "initial_prompt": prompt,
                "df_summary": df_summary,
                "df_sample": df_sample,
                "max_rows": MAX_ROWS
            }
            analysis_response = invoke_model(USER_ROLE_TEMPLATE_ANALYSIS, analysis_prompt)
            st.write(analysis_response)
            msgs.add_ai_message(f"[{timestamp}] {analysis_response}")
            logger.info("Etapa 2 analysis completed:\n%s", analysis_response)

            # Agregar mensaje de finalizaci√≥n y disponibilidad del chat
            st.chat_message("ai", avatar="ü§ñ").write(f"**{timestamp}** - ‚úÖ **Consulta finalizada.** El chat est√° nuevamente disponible. Por favor, ingrese su pr√≥xima consulta.")
            msgs.add_ai_message(f"[{timestamp}] ‚úÖ **Consulta finalizada.** El chat est√° nuevamente disponible. Por favor, ingrese su pr√≥xima consulta.")
            
            # Forzar un rerun de la p√°gina para reiniciar el chat input
            st.rerun()

        except Exception as e:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            error_message = f"[{timestamp}] Error al procesar la consulta: {str(e)}"
            st.chat_message("ai", avatar="ü§ñ").write(f"**{timestamp}** - Error: {str(e)}")
            msgs.add_ai_message(error_message)
            logger.error(f"Error al procesar la consulta: {str(e)}")

            # Agregar mensaje de finalizaci√≥n incluso en caso de error
            st.chat_message("ai", avatar="ü§ñ").write(f"**{timestamp}** - ‚úÖ **Consulta finalizada (con error).** El chat est√° nuevamente disponible. Por favor, ingrese su pr√≥xima consulta.")
            msgs.add_ai_message(f"[{timestamp}] ‚úÖ **Consulta finalizada (con error).** El chat est√° nuevamente disponible. Por favor, ingrese su pr√≥xima consulta.")

df = pd.read_csv(TEMP_CSV_PATH)
logger.info(f"CSV TEMPORAL: {TEMP_CSV_PATH}, Registros Cargados:  {len(df)}")