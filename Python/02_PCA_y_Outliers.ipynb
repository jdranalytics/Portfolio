{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANÁLISIS DE DATOS EXTREMOS (OUTLIERS) Y PCA (PRINCIPAL COMPONENT ANALYSIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.covariance import MinCovDet\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import anderson\n",
    "import seaborn as sns\n",
    "\n",
    "# Cargar el dataset ajustado\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ringoquimico/Portfolio/refs/heads/main/Data%20Sources/call_center_data.csv', sep=';', quotechar='\"')\n",
    "\n",
    "# Codificar variables categóricas\n",
    "categorical_vars = ['channel', 'classification', 'resolved_in_sla', 'first_touch_resolution', 'csat_rated_group_name', 'issue_classification']\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_vars, drop_first=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['case_id', 'date', 'start_of_week', 'start_of_month',\n",
       "       'translated_comments', 'sentiment', 'sentiment_rate', 'channel',\n",
       "       'resolution_time_min', 'csat_rating_received', 'classification',\n",
       "       'resolved_in_sla', 'first_touch_resolution', 'group_name_history',\n",
       "       'groups', 'total_groups', 'csat_rated_group_name',\n",
       "       'issue_classification'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['case_id', 'date', 'start_of_week', 'start_of_month',\n",
       "       'translated_comments', 'sentiment', 'sentiment_rate',\n",
       "       'resolution_time_min', 'csat_rating_received', 'group_name_history',\n",
       "       'groups', 'total_groups', 'channel_chatbot', 'channel_email',\n",
       "       'channel_phone', 'classification_DETRACTOR', 'classification_PROMOTER',\n",
       "       'resolved_in_sla_0', 'resolved_in_sla_1', 'first_touch_resolution_0',\n",
       "       'first_touch_resolution_1', 'csat_rated_group_name_Grupo A',\n",
       "       'csat_rated_group_name_Grupo B', 'csat_rated_group_name_Grupo C',\n",
       "       'csat_rated_group_name_Grupo D', 'csat_rated_group_name_Grupo E',\n",
       "       'csat_rated_group_name_Grupo F', 'csat_rated_group_name_Grupo G',\n",
       "       'csat_rated_group_name_Grupo H', 'csat_rated_group_name_Grupo I',\n",
       "       'csat_rated_group_name_Grupo J', 'issue_classification_Account Setup',\n",
       "       'issue_classification_Balance Inquiry',\n",
       "       'issue_classification_Card Issues', 'issue_classification_Fees Inquiry',\n",
       "       'issue_classification_Fraud Report',\n",
       "       'issue_classification_General Inquiry',\n",
       "       'issue_classification_Loan Inquiry',\n",
       "       'issue_classification_Login Problems',\n",
       "       'issue_classification_Payment Issues',\n",
       "       'issue_classification_Transaction Error'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OUTLIERS Y PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'aht'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key, method, tolerance)\u001b[39m\n\u001b[32m   3801\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3802\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3803\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'aht'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Definir variables numéricas para PCA (incluye diferencia para detectar tiempos muertos)\u001b[39;00m\n\u001b[32m      2\u001b[39m df_clean = df_encoded.copy()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df_clean[\u001b[33m'\u001b[39m\u001b[33mdead_time\u001b[39m\u001b[33m'\u001b[39m] = df_clean[\u001b[33m'\u001b[39m\u001b[33mresolution_time_min\u001b[39m\u001b[33m'\u001b[39m] - \u001b[43mdf_clean\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maht\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      4\u001b[39m var_list = [\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mresolution_time_min\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcsat_rating_received\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msentiment_rate\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtotal_groups\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33maht\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtalk_time\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhold_time\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwrap_up_time\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdead_time\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mchannel_phone\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mchannel_email\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mchannel_chatbot\u001b[39m\u001b[33m'\u001b[39m , \u001b[33m'\u001b[39m\u001b[33mresolved_in_sla_1\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mresolved_in_sla_0\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfirst_touch_resolution_True\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      8\u001b[39m ]\n\u001b[32m      9\u001b[39m id_columns = [\u001b[33m'\u001b[39m\u001b[33mcase_id\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:3807\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3805\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   3806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m3807\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3808\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   3809\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key, method, tolerance)\u001b[39m\n\u001b[32m   3802\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._engine.get_loc(casted_key)\n\u001b[32m   3803\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m-> \u001b[39m\u001b[32m3804\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3805\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3806\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3808\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3809\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'aht'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Definir variables numéricas para PCA (incluye diferencia para detectar tiempos muertos)\n",
    "df_clean = df_encoded.copy()\n",
    "df_clean['dead_time'] = df_clean['resolution_time_min'] - df_clean['aht']\n",
    "var_list = [\n",
    "    'resolution_time_min', 'csat_rating_received', 'sentiment_rate', 'total_groups',\n",
    "    'aht', 'talk_time', 'hold_time', 'wrap_up_time', 'dead_time',\n",
    "    'channel_phone', 'channel_email', 'channel_chatbot' , 'resolved_in_sla_1', 'resolved_in_sla_0', 'first_touch_resolution_True'\n",
    "]\n",
    "id_columns = ['case_id']\n",
    "\n",
    "# Eliminar NaN y preparar datos\n",
    "df_clean = df_clean.dropna(subset=id_columns + var_list)\n",
    "X = df_clean[var_list].values\n",
    "\n",
    "# Verificar el número de muestras\n",
    "print(f\"Número de muestras después de eliminar NaN: {X.shape[0]}\")\n",
    "n_samples = X.shape[0]\n",
    "n_outliers_expected = int(n_samples * 0.1)\n",
    "print(f\"Número esperado de outliers (10% de contaminación): {n_outliers_expected}\")\n",
    "\n",
    "# Estandarizar los datos\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Verificar normalidad con Anderson-Darling\n",
    "print(\"\\nPrueba de normalidad (Anderson-Darling) por variable:\")\n",
    "is_gaussian = []\n",
    "for i, var in enumerate(var_list):\n",
    "    result = anderson(X_scaled[:, i])\n",
    "    is_gaussian_var = result.statistic < result.critical_values[2]\n",
    "    is_gaussian.append(is_gaussian_var)\n",
    "    print(f\"{var}: statistic = {result.statistic:.4f}, critical value (5%) = {result.critical_values[2]:.4f}, Gaussian = {is_gaussian_var}\")\n",
    "\n",
    "# Decidir método\n",
    "gaussian_count = sum(is_gaussian)\n",
    "total_vars = len(var_list)\n",
    "print(f\"\\nPorcentaje de variables gaussianas: {(gaussian_count / total_vars) * 100:.1f}%\")\n",
    "method = \"MinCovDet\" if (gaussian_count / total_vars) < 0.5 else \"EllipticEnvelope\"\n",
    "print(f\"Método sugerido: {method}\")\n",
    "\n",
    "# Reducir a 2D con PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Loadings\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "print(\"\\nContribuciones de las variables a las Componentes Principales (loadings):\")\n",
    "for i, var in enumerate(var_list):\n",
    "    print(f\"{var}: PC1 = {loadings[i, 0]:.4f}, PC2 = {loadings[i, 1]:.4f}\")\n",
    "print(f\"Porcentaje de varianza explicada: PC1 = {pca.explained_variance_ratio_[0] * 100:.2f}%, PC2 = {pca.explained_variance_ratio_[1] * 100:.2f}%\")\n",
    "\n",
    "pc1_dominant = var_list[np.argmax(np.abs(loadings[:, 0]))]\n",
    "pc2_dominant = var_list[np.argmax(np.abs(loadings[:, 1]))]\n",
    "print(f\"\\nPC1 está dominada principalmente por: {pc1_dominant}\")\n",
    "print(f\"PC2 está dominada principalmente por: {pc2_dominant}\")\n",
    "\n",
    "# Detección de outliers con MinCovDet\n",
    "model = MinCovDet(random_state=42).fit(X_2d)\n",
    "mahal_distances = model.mahalanobis(X_2d)\n",
    "threshold = np.percentile(mahal_distances, 100 * (1 - 0.1))\n",
    "outlier_indices = np.where(mahal_distances > threshold)[0]\n",
    "\n",
    "# Tukey\n",
    "q1_x = np.percentile(X_2d[:, 0], 25)\n",
    "q3_x = np.percentile(X_2d[:, 0], 75)\n",
    "iqr_x = q3_x - q1_x\n",
    "lower_bound_x = q1_x - 1.5 * iqr_x\n",
    "upper_bound_x = q3_x + 1.5 * iqr_x\n",
    "\n",
    "q1_y = np.percentile(X_2d[:, 1], 25)\n",
    "q3_y = np.percentile(X_2d[:, 1], 75)\n",
    "iqr_y = q3_y - q1_y\n",
    "lower_bound_y = q1_y - 1.5 * iqr_y\n",
    "upper_bound_y = q3_y + 1.5 * iqr_y\n",
    "\n",
    "tukey_outliers_x = (X_2d[:, 0] < lower_bound_x) | (X_2d[:, 0] > upper_bound_x)\n",
    "tukey_outliers_y = (X_2d[:, 1] < lower_bound_y) | (X_2d[:, 1] > upper_bound_y)\n",
    "tukey_outlier_indices = np.where(tukey_outliers_x | tukey_outliers_y)[0]\n",
    "\n",
    "outlier_types_tukey = []\n",
    "for idx in tukey_outlier_indices:\n",
    "    x_val = X_2d[idx, 0]\n",
    "    y_val = X_2d[idx, 1]\n",
    "    if x_val > upper_bound_x or y_val > upper_bound_y:\n",
    "        outlier_types_tukey.append(\"Superior\")\n",
    "    elif x_val < lower_bound_x or y_val < lower_bound_y:\n",
    "        outlier_types_tukey.append(\"Inferior\")\n",
    "    else:\n",
    "        outlier_types_tukey.append(\"Unknown\")\n",
    "\n",
    "# Preparar DataFrames\n",
    "df_clean = df_clean.reset_index(drop=True)\n",
    "df_clean['Key'] = df_clean['case_id'].astype(str)\n",
    "df_outliers = df_clean.iloc[outlier_indices].copy()\n",
    "df_outliers['Tipo_Outlier'] = ['Superior' if mahal_distances[idx] > threshold else 'Inferior' for idx in outlier_indices]\n",
    "df_outliers['Metodo_Seleccionado'] = 'MinCovDet'\n",
    "\n",
    "df_outliers_tukey = df_clean.iloc[tukey_outlier_indices].copy()\n",
    "df_outliers_tukey['Tipo_Outlier'] = outlier_types_tukey\n",
    "df_outliers_tukey['Metodo_Seleccionado'] = 'Tukey'\n",
    "\n",
    "# Visualización\n",
    "plt.figure(figsize=(12, 8))\n",
    "inliers = X_2d[mahal_distances <= threshold]\n",
    "outliers_method = X_2d[outlier_indices]\n",
    "outliers_tukey = X_2d[tukey_outlier_indices]\n",
    "plt.scatter(inliers[:, 0], inliers[:, 1], color='black', label='Inliers')\n",
    "plt.scatter(outliers_method[:, 0], outliers_method[:, 1], color='red', label='Outliers (MinCovDet)')\n",
    "plt.scatter(outliers_tukey[:, 0], outliers_tukey[:, 1], color='blue', label='Outliers (Tukey)', alpha=0.5)\n",
    "plt.title(f\"Outlier Detection for Low CSAT Scores (PCA 2D)\\nVarianza explicada: PC1 {pca.explained_variance_ratio_[0]*100:.2f}%, PC2 {pca.explained_variance_ratio_[1]*100:.2f}%\")\n",
    "plt.xlabel(f'PC1 (Dominada por {pc1_dominant})')\n",
    "plt.ylabel(f'PC2 (Dominada por {pc2_dominant})')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Heatmap de correlaciones\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df_clean[var_list].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "plt.title('Correlation Heatmap for CSAT Variables')\n",
    "plt.show()\n",
    "\n",
    "# Exportar resultados\n",
    "df_outliers.to_csv('csat_outliers_adjusted.csv', index=False)\n",
    "df_outliers_tukey.to_csv('csat_outliers_tukey_adjusted.csv', index=False)\n",
    "print(\"\\nArchivos 'csat_outliers_adjusted.csv' y 'csat_outliers_tukey_adjusted.csv' creados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANÁLISIS DE RESULTADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cargar datos\n",
    "df_clean = pd.read_csv('call_center_data_adjusted.csv', sep=';')\n",
    "var_list = [\n",
    "    'resolution_time_min', 'csat_rating_received', 'sentiment_rate', 'total_groups',\n",
    "    'aht', 'talk_time', 'hold_time', 'wrap_up_time', 'dead_time',\n",
    "    'channel_phone', 'channel_email', 'resolved_in_sla_True', 'first_touch_resolution_True'\n",
    "]\n",
    "id_columns = ['case_id']\n",
    "\n",
    "# Codificar variables categóricas\n",
    "df_encoded = pd.get_dummies(df_clean, columns=['channel', 'classification', 'resolved_in_sla', 'first_touch_resolution', 'csat_rated_group_name', 'issue_classification'], drop_first=True)\n",
    "df_clean = df_encoded.dropna(subset=id_columns + var_list).reset_index(drop=True)\n",
    "df_clean['dead_time'] = df_clean['resolution_time_min'] - df_clean['aht']\n",
    "X = df_clean[var_list].values\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X_scaled)\n",
    "df_outliers = pd.read_csv('csat_outliers_adjusted.csv')\n",
    "outlier_indices = df_outliers.index\n",
    "\n",
    "# Paso 1: Validar causas de CSAT scores bajos\n",
    "print(\"\\n=== Paso 1: Validar causas de CSAT scores bajos ===\")\n",
    "low_csat_outliers = df_outliers[df_outliers['csat_rating_received'] < 3]\n",
    "print(\"\\nOutliers con CSAT scores bajos (< 3):\")\n",
    "print(low_csat_outliers[['Key', 'csat_rating_received', 'resolution_time_min', 'sentiment_rate', 'total_groups', 'dead_time', 'translated_comments']].head(5))\n",
    "\n",
    "correlation_with_csat = df_clean[var_list].corr()['csat_rating_received'].sort_values(ascending=False)\n",
    "print(\"\\nCorrelación de csat_rating_received con otras variables:\")\n",
    "print(correlation_with_csat)\n",
    "main_cause = correlation_with_csat.index[1]\n",
    "print(f\"\\nVariable más correlacionada con CSAT scores bajos (posible causa): {main_cause}\")\n",
    "\n",
    "# Paso 2: Investigar causas secundarias con más componentes\n",
    "print(\"\\n=== Paso 2: Investigar causas secundarias con más componentes ===\")\n",
    "pca_extended = PCA(n_components=0.8)\n",
    "X_pca = pca_extended.fit_transform(X_scaled)\n",
    "print(f\"Número de componentes para el 80% de varianza: {pca_extended.n_components_}\")\n",
    "print(f\"Varianza total explicada: {sum(pca_extended.explained_variance_ratio_)*100:.2f}%\")\n",
    "\n",
    "loadings_pc3 = pca_extended.components_[2]\n",
    "print(\"\\nLoadings de PC3:\")\n",
    "for i, var in enumerate(var_list):\n",
    "    print(f\"{var}: {loadings_pc3[i]:.4f}\")\n",
    "pc3_dominant = var_list[np.argmax(np.abs(loadings_pc3))]\n",
    "print(f\"PC3 está dominada principalmente por: {pc3_dominant}\")\n",
    "\n",
    "q1_pc3 = np.percentile(X_pca[:, 2], 25)\n",
    "q3_pc3 = np.percentile(X_pca[:, 2], 75)\n",
    "iqr_pc3 = q3_pc3 - q1_pc3\n",
    "lower_bound_pc3 = q1_pc3 - 1.5 * iqr_pc3\n",
    "upper_bound_pc3 = q3_pc3 + 1.5 * iqr_pc3\n",
    "outliers_pc3 = (X_pca[:, 2] < lower_bound_pc3) | (X_pca[:, 2] > upper_bound_pc3)\n",
    "outliers_pc3_indices = np.where(outliers_pc3)[0]\n",
    "print(f\"\\nNúmero de outliers en PC3: {len(outliers_pc3_indices)}\")\n",
    "\n",
    "df_outliers_pc3 = df_clean.iloc[outliers_pc3_indices].copy()\n",
    "df_outliers_pc3['Tipo_Outlier'] = np.where(X_pca[outliers_pc3_indices, 2] > upper_bound_pc3, 'Superior', 'Inferior')\n",
    "print(\"\\nOutliers en PC3 (primeras filas):\")\n",
    "print(df_outliers_pc3[['Key', pc3_dominant, 'csat_rating_received', 'translated_comments']].head())\n",
    "\n",
    "# Paso 3: Segmentar por PC2\n",
    "print(\"\\n=== Paso 3: Segmentar outliers por PC2 ===\")\n",
    "pc2_threshold_high = np.percentile(X_2d[:, 1], 75)\n",
    "pc2_threshold_low = np.percentile(X_2d[:, 1], 25)\n",
    "\n",
    "high_impact = df_outliers[X_2d[outlier_indices, 1] > pc2_threshold_high]\n",
    "low_impact = df_outliers[X_2d[outlier_indices, 1] < pc2_threshold_low]\n",
    "\n",
    "print(\"\\nOutliers con alta influencia de PC2 (posiblemente canales problemáticos):\")\n",
    "print(high_impact[['Key', 'csat_rating_received', pc2_dominant, 'translated_comments']].head())\n",
    "print(\"\\nOutliers con baja influencia de PC2:\")\n",
    "print(low_impact[['Key', 'csat_rating_received', pc2_dominant, 'translated_comments']].head())\n",
    "\n",
    "avg_csat_high = high_impact['csat_rating_received'].mean()\n",
    "avg_csat_low = low_impact['csat_rating_received'].mean()\n",
    "print(f\"\\nCSAT promedio en casos de alta influencia (PC2): {avg_csat_high:.2f}\")\n",
    "print(f\"CSAT promedio en casos de baja influencia (PC2): {avg_csat_low:.2f}\")\n",
    "\n",
    "# Paso 4: Estrategias para mejorar CSAT\n",
    "print(\"\\n=== Paso 4: Estrategias para mejorar CSAT scores ===\")\n",
    "print(\"Conclusión basada en datos:\")\n",
    "print(f\"- La variable más correlacionada con CSAT scores bajos es: {main_cause}.\")\n",
    "print(f\"- PC3, dominada por {pc3_dominant}, sugiere causas secundarias (como tiempos muertos si es 'dead_time').\")\n",
    "print(f\"- Los casos con alta influencia de PC2 (probablemente {pc2_dominant}) tienen un CSAT promedio de {avg_csat_high:.2f}, vs. {avg_csat_low:.2f} en baja influencia.\")\n",
    "\n",
    "print(\"\\nRecomendaciones:\")\n",
    "print(f\"1. Prioriza reducir {main_cause}: Si es 'resolution_time_min' o 'dead_time', optimiza los procesos para eliminar tiempos muertos; si es 'total_groups', minimiza transferencias.\")\n",
    "print(f\"2. Aborda causas secundarias: Analiza outliers en PC3 ({pc3_dominant}) para problemas como 'hold_time' o excesivo 'dead_time'.\")\n",
    "print(f\"3. Enfócate en canales de alta influencia: Si {pc2_dominant} es 'channel_phone', mejora el soporte telefónico.\")\n",
    "print(\"4. Monitorea casos con CSAT bajo: Usa los archivos CSV para seguimiento y analiza los 'translated_comments' para identificar patrones cualitativos.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
